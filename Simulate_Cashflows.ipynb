{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840d1e04",
   "metadata": {},
   "source": [
    "# Cashflow Simulation (Fitted Models)\n",
    "\n",
    "Lightweight simulator using fitted timing + ratio distributions, grade transitions, and copula.\n",
    "Includes calibrated omega if `omega_selected.csv` is available, and MSCI projection\n",
    "logic copied from `msci_projection.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31eb3a15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T22:16:54.801235Z",
     "iopub.status.busy": "2026-02-01T22:16:54.800480Z",
     "iopub.status.idle": "2026-02-01T22:16:56.984691Z",
     "shell.execute_reply": "2026-02-01T22:16:56.983875Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "from math import sqrt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9dd378",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T22:16:56.987463Z",
     "iopub.status.busy": "2026-02-01T22:16:56.987091Z",
     "iopub.status.idle": "2026-02-01T22:16:57.000566Z",
     "shell.execute_reply": "2026-02-01T22:16:56.999712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using INPUT_PATH: /Users/mozeramozali/Desktop/Equity-Cashflow-projection/anonymized.csv\n",
      "Using MSCI_PATH: /Users/mozeramozali/Desktop/Equity-Cashflow-projection/MSCI.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "RUN_TAG = os.environ.get(\"RUN_TAG\")\n",
    "HIST_END = os.environ.get(\"HIST_END\")\n",
    "if RUN_TAG is None:\n",
    "    RUN_TAG = HIST_END or \"2025Q3\"\n",
    "BASE_OUT = Path(\"model_fits\") / \"runs\" / RUN_TAG\n",
    "CALIB_DIR = BASE_OUT / \"calibration\"\n",
    "PROJ_DIR = BASE_OUT / \"projection\"\n",
    "\n",
    "INPUT_PATH = \"anonymized.csv\"\n",
    "INPUT_PATH = os.environ.get(\"INPUT_PATH\", INPUT_PATH)\n",
    "FIT_DIR = str(CALIB_DIR)\n",
    "FIT_DIR = os.environ.get(\"FIT_DIR\", FIT_DIR)\n",
    "COPULA_PATH = str(CALIB_DIR / \"copula_params.json\")\n",
    "\n",
    "START_FROM_HIST_END = True  # if HIST_END provided, start projection from that quarter\n",
    "START_FROM_HIST_END = (os.environ.get(\"START_FROM_HIST_END\", str(START_FROM_HIST_END)).lower() in (\"1\",\"true\",\"yes\",\"y\"))\n",
    "\n",
    "\n",
    "START_YEAR = 2025\n",
    "START_QUARTER = \"Q3\"\n",
    "START_YEAR = int(os.environ.get(\"START_YEAR\", START_YEAR))\n",
    "START_QUARTER = os.environ.get(\"START_QUARTER\", START_QUARTER)\n",
    "HORIZON_Q = 20\n",
    "N_SIMS = 1000\n",
    "N_SIMS = int(os.environ.get(\"N_SIMS\", N_SIMS))\n",
    "DRAW_RATIO_CAP = 1.0  # allow cumulative draw ratio > 1 (recallables)\n",
    "SEED = 1234\n",
    "\n",
    "OMEGA_MODE = \"calibrated\"  # \"none\", \"global\", or \"calibrated\"\n",
    "OMEGA_CLIP = (-0.5, 0.5)  # hard cap to avoid explosive NAV\n",
    "\n",
    "MSCI_PATH = \"msci.xlsx\"\n",
    "MSCI_MODE = \"unconditional\"  # enforced: always use projected MSCI paths\n",
    "MSCI_SCENARIO = \"neutral\"     # bullish / neutral / bearish\n",
    "REP_CAP_P90 = os.environ.get(\"REP_CAP_P90\", \"0\").lower() in (\"1\",\"true\",\"yes\",\"y\")\n",
    "PACE_SCALE = float(os.environ.get(\"PACE_SCALE\", \"1.0\"))\n",
    "AUTO_PACE_ONEPASS = os.environ.get(\"AUTO_PACE_ONEPASS\", \"true\").lower() in (\"1\",\"true\",\"yes\",\"y\")\n",
    "PACE_CALIB_N_Q = int(os.environ.get(\"PACE_CALIB_N_Q\", \"0\"))\n",
    "if PACE_CALIB_N_Q <= 0:\n",
    "    PACE_CALIB_N_Q = HORIZON_Q\n",
    "MSCI_TILT_STRENGTH = 1.15\n",
    "\n",
    "if not Path(INPUT_PATH).exists():\n",
    "    candidates = list(Path.cwd().glob(\"**/anonymized.csv\"))\n",
    "    if not candidates:\n",
    "        candidates = list(Path.cwd().parent.glob(\"**/anonymized.csv\"))\n",
    "    if not candidates:\n",
    "        candidates = list(Path.cwd().parent.parent.glob(\"**/anonymized.csv\"))\n",
    "    if candidates:\n",
    "        INPUT_PATH = str(candidates[0])\n",
    "    else:\n",
    "        raise FileNotFoundError(\"anonymized.csv not found. Set INPUT_PATH to the full path.\")\n",
    "\n",
    "# Prefer MSCI.xlsx if present\n",
    "if not Path(MSCI_PATH).exists():\n",
    "    candidates = []\n",
    "    for p in [\"MSCI.xlsx\", \"msci.xlsx\"]:\n",
    "        if Path(p).exists():\n",
    "            candidates.append(Path(p))\n",
    "    if not candidates:\n",
    "        for base in [Path.cwd(), Path.cwd().parent, Path.cwd().parent.parent]:\n",
    "            for p in [\"MSCI.xlsx\", \"msci.xlsx\"]:\n",
    "                cand = base / p\n",
    "                if cand.exists():\n",
    "                    candidates.append(cand)\n",
    "    if candidates:\n",
    "        MSCI_PATH = str(candidates[0])\n",
    "\n",
    "print(\"Using INPUT_PATH:\", INPUT_PATH)\n",
    "print(\"Using MSCI_PATH:\", MSCI_PATH)\n",
    "\n",
    "# Auto-resolve FIT_DIR if outputs were written under model_fits/model_fits/outputs\n",
    "if not Path(FIT_DIR).exists():\n",
    "    alt = Path(\"model_fits/model_fits/outputs\")\n",
    "    if alt.exists():\n",
    "        FIT_DIR = str(alt)\n",
    "\n",
    "\n",
    "# Ensure FIT_DIR points to an existing calibration folder\n",
    "def _resolve_fit_dir(fit_dir: str) -> str:\n",
    "    cand = Path(fit_dir)\n",
    "    if (cand / \"ratio_fit_selected.csv\").exists():\n",
    "        return str(cand)\n",
    "    # candidate locations relative to project and notebook dirs\n",
    "    bases = [Path.cwd(), Path.cwd().parent]\n",
    "    for base in bases:\n",
    "        for p in [\n",
    "            base / \"model_fits\" / \"runs\" / RUN_TAG / \"calibration\",\n",
    "            base / \"model_fits\" / \"model_fits\" / \"runs\" / RUN_TAG / \"calibration\",\n",
    "        ]:\n",
    "            if (p / \"ratio_fit_selected.csv\").exists():\n",
    "                return str(p)\n",
    "    # fallback: first calibration folder with ratio_fit_selected.csv\n",
    "    hits = list(Path.cwd().glob(\"**/ratio_fit_selected.csv\"))\n",
    "    if hits:\n",
    "        return str(hits[0].parent)\n",
    "    return fit_dir\n",
    "\n",
    "FIT_DIR = _resolve_fit_dir(FIT_DIR)\n",
    "\n",
    "Path(FIT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Path(CALIB_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(PROJ_DIR).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b24f74ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T22:16:57.003094Z",
     "iopub.status.busy": "2026-02-01T22:16:57.002793Z",
     "iopub.status.idle": "2026-02-01T22:16:57.094741Z",
     "shell.execute_reply": "2026-02-01T22:16:57.093943Z"
    }
   },
   "outputs": [],
   "source": [
    "AGE_BINS_Q = [-1, 3, 7, 11, 15, 19, 1000]\n",
    "AGE_LABELS = [\"0-3\", \"4-7\", \"8-11\", \"12-15\", \"16-19\", \"20+\"]\n",
    "AGE_BUCKET_ORDER = {a: i for i, a in enumerate(AGE_LABELS)}\n",
    "GRADE_STATES = [\"A\", \"B\", \"C\", \"D\"]\n",
    "GRADE_ANCHOR_Q = 20  # anchor initial grade for first 5 years (20 quarters)\n",
    "GRADE_UPDATE_ENABLED = os.environ.get(\"GRADE_UPDATE_ENABLED\", \"1\").lower() in (\"1\", \"true\", \"yes\", \"y\")\n",
    "SMALL_SAMPLE_RULE_ENABLED = os.environ.get(\"SMALL_SAMPLE_RULE_ENABLED\", \"1\").lower() in (\"1\", \"true\", \"yes\", \"y\")\n",
    "\n",
    "\n",
    "def _norm_key(s: str) -> str:\n",
    "    return \" \".join(s.strip().lower().replace(\"_\", \" \").split())\n",
    "\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    col_map = {_norm_key(c): c for c in df.columns}\n",
    "    def _get(name: str) -> str:\n",
    "        k = _norm_key(name)\n",
    "        return col_map.get(k, name)\n",
    "\n",
    "    rename = {}\n",
    "    rename[_get(\"Adj strategy\")] = \"Adj Strategy\"\n",
    "    rename[_get(\"Adj Strategy\")] = \"Adj Strategy\"\n",
    "    rename[_get(\"Quarter of Transaction Date\")] = \"Quarter\"\n",
    "    rename[_get(\"Year of Transaction Date\")] = \"Year\"\n",
    "    rename[_get(\"FundID\")] = \"FundID\"\n",
    "    rename[_get(\"Grade\")] = \"Grade\"\n",
    "    rename[_get(\"Current Grade\")] = \"Grade_Current\"\n",
    "    rename[_get(\"CurrentGrade\")] = \"Grade_Current\"\n",
    "    rename[_get(\"Grade Current\")] = \"Grade_Current\"\n",
    "    rename[_get(\"Grade_Current\")] = \"Grade_Current\"\n",
    "    rename[_get(\"Adj Drawdown EUR\")] = \"Adj Drawdown EUR\"\n",
    "    rename[_get(\"Adj Repayment EUR\")] = \"Adj Repayment EUR\"\n",
    "    rename[_get(\"Recallable\")] = \"Recallable\"\n",
    "    rename[_get(\"NAV Adjusted EUR\")] = \"NAV Adjusted EUR\"\n",
    "    rename[_get(\"Commitment EUR\")] = \"Commitment EUR\"\n",
    "    rename[_get(\"Fund Workflow Stage\")] = \"Fund Workflow Stage\"\n",
    "    rename[_get(\"Planned End Date\")] = \"Planned End Date\"\n",
    "    rename[_get(\"Planned end date with add. years as per legal doc\")] = \"Planned End Date\"\n",
    "    rename[_get(\"Planned End Date as per legal documentation\")] = \"Planned End Date\"\n",
    "    rename[_get(\"Signed Amount EUR\")] = \"Signed Amount EUR\"\n",
    "    rename[_get(\"Capacity\")] = \"Capacity\"\n",
    "    rename[_get(\"Fund_Age_Quarters\")] = \"Fund_Age_Quarters\"\n",
    "    rename[_get(\"draw_cum_prev\")] = \"draw_cum_prev\"\n",
    "    rename[_get(\"Recallable_Percentage_Decimal\")] = \"Recallable_Percentage_Decimal\"\n",
    "    rename[_get(\"Expiration_Quarters\")] = \"Expiration_Quarters\"\n",
    "    return df.rename(columns=rename)\n",
    "\n",
    "\n",
    "def parse_quarter(q) -> float:\n",
    "    if pd.isna(q):\n",
    "        return np.nan\n",
    "    if isinstance(q, (int, np.integer, float, np.floating)):\n",
    "        return float(q)\n",
    "    s = str(q).strip().upper()\n",
    "    if s.startswith(\"Q\"):\n",
    "        s = s[1:]\n",
    "    try:\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def add_quarter_end(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"Quarter\"] = df[\"Quarter\"].apply(parse_quarter)\n",
    "    df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\")\n",
    "    m = df[\"Year\"].notna() & df[\"Quarter\"].notna()\n",
    "    years = df.loc[m, \"Year\"].astype(int)\n",
    "    quarters = df.loc[m, \"Quarter\"].astype(int)\n",
    "    df.loc[m, \"quarter_end\"] = pd.PeriodIndex(year=years, quarter=quarters, freq=\"Q\").to_timestamp(\"Q\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_current_grade(df: pd.DataFrame, context: str = \"\") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    if \"Grade\" in df.columns and \"Grade_Seed\" not in df.columns:\n",
    "        df[\"Grade_Seed\"] = df[\"Grade\"]\n",
    "\n",
    "    if \"Grade\" in df.columns:\n",
    "        df[\"Grade\"] = df[\"Grade\"].astype(str).str.strip()\n",
    "        df.loc[df[\"Grade\"].isin([\"\", \"nan\", \"None\", \"NaN\", \"<NA>\"]), \"Grade\"] = np.nan\n",
    "\n",
    "    if \"quarter_end\" not in df.columns:\n",
    "        df = add_quarter_end(df)\n",
    "\n",
    "    # Optionally bypass grading logic and use provided Grade as-is\n",
    "    if os.environ.get(\"GRADE_USE_INPUT\", \"0\").lower() in (\"1\", \"true\", \"yes\", \"y\"):\n",
    "        if \"Grade\" in df.columns:\n",
    "            df[\"Grade\"] = df[\"Grade\"].astype(str).str.strip()\n",
    "            df.loc[df[\"Grade\"].isin([\"\", \"nan\", \"None\", \"NaN\", \"<NA>\"]), \"Grade\"] = np.nan\n",
    "            if \"quarter_end\" not in df.columns:\n",
    "                df = add_quarter_end(df)\n",
    "            df = df.sort_values([\"FundID\", \"quarter_end\"])\n",
    "            df[\"Grade_Current\"] = df.groupby(\"FundID\")[\"Grade\"].ffill()\n",
    "            df[\"Grade\"] = df[\"Grade_Current\"]\n",
    "        if context:\n",
    "            print(f\"Using provided Grade for {context}.\")\n",
    "        return df\n",
    "\n",
    "    df[\"QPeriod\"] = df[\"quarter_end\"].dt.to_period(\"Q\")\n",
    "\n",
    "    cols = [\n",
    "        \"FundID\",\n",
    "        \"Adj Strategy\",\n",
    "        \"QPeriod\",\n",
    "        \"quarter_end\",\n",
    "        \"Adj Drawdown EUR\",\n",
    "        \"Adj Repayment EUR\",\n",
    "        \"NAV Adjusted EUR\",\n",
    "        \"First Closing Date\",\n",
    "        \"Grade\",\n",
    "    ]\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    cash = df[cols].copy()\n",
    "    cash = cash.rename(\n",
    "        columns={\n",
    "            \"Adj Strategy\": \"AdjStrategy\",\n",
    "            \"quarter_end\": \"TransactionDate\",\n",
    "            \"First Closing Date\": \"FirstClosingDate\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    cash[\"TransactionDate\"] = pd.to_datetime(cash[\"TransactionDate\"], errors=\"coerce\")\n",
    "    if \"FirstClosingDate\" in cash.columns:\n",
    "        cash[\"FirstClosingDate\"] = pd.to_datetime(cash[\"FirstClosingDate\"], errors=\"coerce\")\n",
    "    else:\n",
    "        cash[\"FirstClosingDate\"] = pd.NaT\n",
    "\n",
    "    for c in [\"Adj Drawdown EUR\", \"Adj Repayment EUR\", \"NAV Adjusted EUR\"]:\n",
    "        if c in cash.columns:\n",
    "            cash[c] = pd.to_numeric(cash[c], errors=\"coerce\").fillna(0.0)\n",
    "        else:\n",
    "            cash[c] = 0.0\n",
    "\n",
    "    cash = cash.dropna(subset=[\"FundID\", \"TransactionDate\"])\n",
    "\n",
    "    if cash[\"FirstClosingDate\"].isna().any():\n",
    "        first_tx = cash.groupby(\"FundID\")[\"TransactionDate\"].transform(\"min\")\n",
    "        cash[\"FirstClosingDate\"] = cash[\"FirstClosingDate\"].fillna(first_tx)\n",
    "\n",
    "    if cash.empty:\n",
    "        return df\n",
    "\n",
    "    fund_strategy = (\n",
    "        cash.groupby(\"FundID\")[\"AdjStrategy\"]\n",
    "        .agg(lambda s: s.mode().iat[0] if len(s.mode()) else s.iloc[0])\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    q_snap = (\n",
    "        cash.sort_values([\"FundID\", \"TransactionDate\"])\n",
    "        .groupby([\"FundID\", \"QPeriod\"], as_index=False)\n",
    "        .agg(\n",
    "            AdjStrategy=(\"AdjStrategy\", \"last\"),\n",
    "            FirstClosingDate=(\"FirstClosingDate\", \"last\"),\n",
    "            QuarterDrawdown=(\"Adj Drawdown EUR\", \"sum\"),\n",
    "            QuarterRepayment=(\"Adj Repayment EUR\", \"sum\"),\n",
    "            QuarterEndNAV=(\"NAV Adjusted EUR\", \"last\"),\n",
    "            QuarterEndDate=(\"TransactionDate\", \"max\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    q_snap = q_snap.sort_values([\"FundID\", \"QPeriod\"])\n",
    "    q_snap[\"QuarterEndNAV\"] = q_snap.groupby(\"FundID\")[\"QuarterEndNAV\"].ffill().fillna(0)\n",
    "\n",
    "    q_snap[\"CumDrawdown\"] = q_snap.groupby(\"FundID\")[\"QuarterDrawdown\"].cumsum()\n",
    "    q_snap[\"CumRepayment\"] = q_snap.groupby(\"FundID\")[\"QuarterRepayment\"].cumsum()\n",
    "    q_snap[\"PaidIn\"] = q_snap[\"CumDrawdown\"].abs()\n",
    "    q_snap[\"Distributed\"] = q_snap[\"CumRepayment\"].abs()\n",
    "    q_snap[\"NAV\"] = q_snap[\"QuarterEndNAV\"].abs()\n",
    "\n",
    "    q_snap[\"DPI\"] = np.where(q_snap[\"PaidIn\"] > 0, q_snap[\"Distributed\"] / q_snap[\"PaidIn\"], np.nan)\n",
    "    q_snap[\"TVPI\"] = np.where(\n",
    "        q_snap[\"PaidIn\"] > 0,\n",
    "        (q_snap[\"Distributed\"] + q_snap[\"NAV\"]) / q_snap[\"PaidIn\"],\n",
    "        np.nan,\n",
    "    )\n",
    "\n",
    "    def xnpv(rate, cfs, dts):\n",
    "        dts = np.asarray(dts, dtype=\"datetime64[ns]\")\n",
    "        cfs = np.asarray(cfs, dtype=float)\n",
    "        t0 = dts[0]\n",
    "        day_counts = (dts - t0) / np.timedelta64(1, \"D\")\n",
    "        years = day_counts / 365.0\n",
    "        return np.sum(cfs / ((1.0 + rate) ** years))\n",
    "\n",
    "    def xirr_newton(cfs, dts, guess=0.1, max_iter=80, tol=1e-7):\n",
    "        dts = np.asarray(dts, dtype=\"datetime64[ns]\")\n",
    "        cfs = np.asarray(cfs, dtype=float)\n",
    "        rate = float(guess)\n",
    "        for _ in range(max_iter):\n",
    "            f = xnpv(rate, cfs, dts)\n",
    "            if not np.isfinite(f):\n",
    "                return np.nan\n",
    "            if abs(f) < tol:\n",
    "                return rate\n",
    "            eps = 1e-6\n",
    "            f1 = xnpv(rate + eps, cfs, dts)\n",
    "            df = (f1 - f) / eps\n",
    "            if df == 0 or not np.isfinite(df):\n",
    "                return np.nan\n",
    "            rate_new = rate - f / df\n",
    "            if rate_new <= -0.999999 or not np.isfinite(rate_new):\n",
    "                return np.nan\n",
    "            rate = rate_new\n",
    "        return np.nan\n",
    "\n",
    "    def compute_fund_quarter_xirr(fund_cash, fund_q):\n",
    "        fund_cash = fund_cash.sort_values(\"TransactionDate\")\n",
    "        cfs = (-fund_cash[\"Adj Drawdown EUR\"].abs() + fund_cash[\"Adj Repayment EUR\"].abs()).to_numpy(dtype=float)\n",
    "        dts = fund_cash[\"TransactionDate\"].to_numpy(dtype=\"datetime64[ns]\")\n",
    "        irr_vals = []\n",
    "        irr_flags = []\n",
    "        j = 0\n",
    "        n = len(cfs)\n",
    "        for r in fund_q.itertuples(index=False):\n",
    "            q_end = np.datetime64(r.QuarterEndDate, \"ns\")\n",
    "            while j < n and dts[j] <= q_end:\n",
    "                j += 1\n",
    "            cfs_slice = cfs[:j]\n",
    "            dts_slice = dts[:j]\n",
    "            if len(cfs_slice) == 0:\n",
    "                irr_vals.append(np.nan)\n",
    "                irr_flags.append(\"no_txns\")\n",
    "                continue\n",
    "            terminal_nav = float(abs(r.NAV)) if np.isfinite(r.NAV) else 0.0\n",
    "            cfs_full = np.append(cfs_slice, terminal_nav)\n",
    "            dts_full = np.append(dts_slice, q_end).astype(\"datetime64[ns]\")\n",
    "            if not (np.any(cfs_full < 0) and np.any(cfs_full > 0)):\n",
    "                irr_vals.append(np.nan)\n",
    "                irr_flags.append(\"no_sign_change\")\n",
    "                continue\n",
    "            tvpi = r.TVPI\n",
    "            guess = 0.10 if (pd.notna(tvpi) and tvpi > 1.0) else -0.10\n",
    "            irr = xirr_newton(cfs_full, dts_full, guess=guess)\n",
    "            if pd.notna(tvpi) and (0.98 <= tvpi <= 1.02):\n",
    "                if not np.isfinite(irr):\n",
    "                    irr2 = xirr_newton(cfs_full, dts_full, guess=-guess)\n",
    "                    if np.isfinite(irr2):\n",
    "                        irr = irr2\n",
    "                        irr_flags.append(\"flat_retry_success\")\n",
    "                    else:\n",
    "                        irr = 0.0\n",
    "                        irr_flags.append(\"flat_to_zero\")\n",
    "                else:\n",
    "                    irr_flags.append(\"flat_success\")\n",
    "            else:\n",
    "                irr_flags.append(\"ok\" if np.isfinite(irr) else \"fail\")\n",
    "            irr_vals.append(irr)\n",
    "        return pd.DataFrame({\"IRR\": irr_vals, \"IRR_Flag\": irr_flags})\n",
    "\n",
    "    irr_rows = []\n",
    "    for fund_id, fund_q in q_snap.groupby(\"FundID\", sort=False):\n",
    "        fund_cash = cash[cash[\"FundID\"] == fund_id]\n",
    "        irr_df = compute_fund_quarter_xirr(fund_cash, fund_q)\n",
    "        irr_df = irr_df.copy()\n",
    "        irr_df[\"FundID\"] = fund_id\n",
    "        irr_df[\"QPeriod\"] = fund_q[\"QPeriod\"].values\n",
    "        irr_rows.append(irr_df)\n",
    "\n",
    "    if irr_rows:\n",
    "        irr_all = pd.concat(irr_rows, ignore_index=True)\n",
    "        q_snap = q_snap.merge(irr_all, on=[\"FundID\", \"QPeriod\"], how=\"left\")\n",
    "    else:\n",
    "        q_snap[\"IRR\"] = np.nan\n",
    "\n",
    "    def quartile_to_grade(s):\n",
    "        r = s.rank(pct=True)\n",
    "        return pd.cut(r, [0, 0.25, 0.5, 0.75, 1], labels=[\"D\", \"C\", \"B\", \"A\"], include_lowest=True)\n",
    "\n",
    "    q_snap[\"Grade_DPI\"] = q_snap.groupby([\"AdjStrategy\", \"QPeriod\"])[\"DPI\"].transform(quartile_to_grade)\n",
    "    q_snap[\"Grade_TVPI\"] = q_snap.groupby([\"AdjStrategy\", \"QPeriod\"])[\"TVPI\"].transform(quartile_to_grade)\n",
    "    q_snap[\"Grade_IRR\"] = q_snap.groupby([\"AdjStrategy\", \"QPeriod\"])[\"IRR\"].transform(quartile_to_grade)\n",
    "\n",
    "    DEBT_STRATEGIES = {\"Hybrid Debt-Equity\", \"Private Debt\", \"Other Private Debt\"}\n",
    "    VC_STRATEGY = \"Venture Capital\"\n",
    "\n",
    "    rep = cash[cash[\"Adj Repayment EUR\"].abs() > 0].copy()\n",
    "    first_repay = rep.groupby(\"FundID\")[\"TransactionDate\"].min().reset_index(name=\"FirstRepaymentDate\")\n",
    "    first_close = cash.groupby(\"FundID\")[\"FirstClosingDate\"].min().reset_index(name=\"FirstCloseDate\")\n",
    "    fund_timing = first_close.merge(first_repay, on=\"FundID\", how=\"left\")\n",
    "    fund_timing = fund_timing.merge(fund_strategy, on=\"FundID\", how=\"left\")\n",
    "    fund_timing[\"RepayWithin5Y\"] = (\n",
    "        fund_timing[\"FirstRepaymentDate\"].notna()\n",
    "        & (fund_timing[\"FirstRepaymentDate\"] <= (fund_timing[\"FirstCloseDate\"] + pd.DateOffset(years=5)))\n",
    "    )\n",
    "    fund_timing[\"BaseYears\"] = np.where(fund_timing[\"RepayWithin5Y\"], 5, 6)\n",
    "    fund_timing[\"InvestPeriodYears\"] = fund_timing[\"BaseYears\"] + 1\n",
    "\n",
    "    q_snap = q_snap.merge(\n",
    "        fund_timing[[\"FundID\", \"FirstCloseDate\", \"InvestPeriodYears\", \"AdjStrategy\"]],\n",
    "        on=[\"FundID\", \"AdjStrategy\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    q_snap[\"IsInvestmentPeriod\"] = q_snap[\"QuarterEndDate\"] <= (\n",
    "        q_snap[\"FirstCloseDate\"] + q_snap[\"InvestPeriodYears\"].apply(lambda y: pd.DateOffset(years=int(y)))\n",
    "    )\n",
    "\n",
    "    fund_counts = (\n",
    "        q_snap.groupby([\"AdjStrategy\", \"QPeriod\"])[\"FundID\"].nunique().rename(\"StrategyFundCount\").reset_index()\n",
    "    )\n",
    "    q_snap = q_snap.merge(fund_counts, on=[\"AdjStrategy\", \"QPeriod\"], how=\"left\")\n",
    "\n",
    "    q_snap[\"IsDebt\"] = q_snap[\"AdjStrategy\"].isin(DEBT_STRATEGIES)\n",
    "    q_snap[\"IsVC\"] = q_snap[\"AdjStrategy\"].eq(VC_STRATEGY)\n",
    "\n",
    "    grade_to_idx = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
    "    idx_to_grade = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "\n",
    "    def worse_grade(g1, g2):\n",
    "        if pd.isna(g1):\n",
    "            return g2\n",
    "        if pd.isna(g2):\n",
    "            return g1\n",
    "        return g1 if grade_to_idx[g1] >= grade_to_idx[g2] else g2\n",
    "\n",
    "    def downgrade_one_notch(g):\n",
    "        if pd.isna(g):\n",
    "            return g\n",
    "        return idx_to_grade[min(grade_to_idx[g] + 1, 3)]\n",
    "\n",
    "    def final_grade(row):\n",
    "        if SMALL_SAMPLE_RULE_ENABLED and row[\"StrategyFundCount\"] < 30:\n",
    "            return worse_grade(row[\"Grade_DPI\"], row[\"Grade_TVPI\"])\n",
    "        if row[\"IsDebt\"]:\n",
    "            return row[\"Grade_IRR\"]\n",
    "        if row[\"IsInvestmentPeriod\"]:\n",
    "            return row[\"Grade_TVPI\"] if row[\"IsVC\"] else row[\"Grade_DPI\"]\n",
    "        base = row[\"Grade_IRR\"]\n",
    "        dpi_g = row[\"Grade_DPI\"]\n",
    "        if pd.isna(base):\n",
    "            return base\n",
    "        if pd.notna(dpi_g) and (grade_to_idx[dpi_g] > grade_to_idx[base]):\n",
    "            return downgrade_one_notch(base)\n",
    "        return base\n",
    "\n",
    "    q_snap[\"CurrentGrade\"] = q_snap.apply(final_grade, axis=1)\n",
    "\n",
    "    fund_quarters = cash[[\"FundID\", \"QPeriod\"]].drop_duplicates().sort_values([\"FundID\", \"QPeriod\"])\n",
    "    fund_quarters[\"RankQ\"] = fund_quarters.groupby(\"FundID\").cumcount() + 1\n",
    "    fund_quarters[\"Block4\"] = (fund_quarters[\"RankQ\"] - 1) // 4 + 1\n",
    "\n",
    "    fund_first = (\n",
    "        cash.dropna(subset=[\"Grade\"]).groupby(\"FundID\", as_index=False).first()[[\"FundID\", \"Grade\"]]\n",
    "        .rename(columns={\"Grade\": \"FirstGrade\"})\n",
    "    )\n",
    "\n",
    "    fund_quarters = fund_quarters.merge(fund_first, on=\"FundID\", how=\"left\")\n",
    "    fund_quarters = fund_quarters.merge(\n",
    "        q_snap[[\"FundID\", \"QPeriod\", \"CurrentGrade\"]], on=[\"FundID\", \"QPeriod\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "    fund_quarters[\"AssignedGrade\"] = np.where(\n",
    "        (fund_quarters[\"RankQ\"] <= 20) & fund_quarters[\"FirstGrade\"].notna(),\n",
    "        fund_quarters[\"FirstGrade\"],\n",
    "        fund_quarters[\"CurrentGrade\"],\n",
    "    )\n",
    "\n",
    "    fund_quarters[\"AssignedGrade\"] = fund_quarters.groupby(\"FundID\")[\"AssignedGrade\"].ffill()\n",
    "\n",
    "    df = df.merge(fund_quarters[[\"FundID\", \"QPeriod\", \"AssignedGrade\"]], on=[\"FundID\", \"QPeriod\"], how=\"left\")\n",
    "    df[\"Grade_Current\"] = df[\"AssignedGrade\"]\n",
    "    if \"Grade_Seed\" in df.columns:\n",
    "        df[\"Grade_Current\"] = df[\"Grade_Current\"].fillna(df[\"Grade_Seed\"])\n",
    "    df[\"Grade\"] = df[\"Grade_Current\"]\n",
    "\n",
    "    if context:\n",
    "        print(f\"Computed Grade_Current using performance rules for {context}.\")\n",
    "\n",
    "    return df\n",
    "    if all(c in df.columns for c in [\"Grade\", \"FundID\", \"quarter_end\"]):\n",
    "        df[\"Grade\"] = df[\"Grade\"].astype(str).str.strip()\n",
    "        df.loc[df[\"Grade\"].isin([\"\", \"nan\", \"None\", \"NaN\", \"<NA>\"]), \"Grade\"] = np.nan\n",
    "        df = df.sort_values([\"FundID\", \"quarter_end\"])\n",
    "        df[\"Grade_Current\"] = df.groupby(\"FundID\")[\"Grade\"].ffill()\n",
    "        df[\"Grade\"] = df[\"Grade_Current\"]\n",
    "        if context:\n",
    "            print(f\"Computed Grade_Current (forward fill) for {context}.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_age_bucket(age_q: int) -> str:\n",
    "    for i in range(len(AGE_BINS_Q) - 1):\n",
    "        if AGE_BINS_Q[i] < age_q <= AGE_BINS_Q[i + 1]:\n",
    "            return AGE_LABELS[i]\n",
    "    return AGE_LABELS[-1]\n",
    "\n",
    "\n",
    "def one_factor_uniforms(n: int, rng: np.random.Generator, rho: float) -> np.ndarray:\n",
    "    Z = rng.standard_normal()\n",
    "    eps = rng.standard_normal(n)\n",
    "    z = rho * Z + sqrt(1.0 - rho * rho) * eps\n",
    "    return stats.norm.cdf(z)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# robust params parser (handles \"np.float64(0.1)\" style)\n",
    "def parse_params(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    if isinstance(val, (list, tuple)):\n",
    "        if len(val) == 1 and isinstance(val[0], (list, tuple)):\n",
    "            return val[0]\n",
    "        return val\n",
    "    if isinstance(val, str):\n",
    "        s = val.strip()\n",
    "        if s == \"\" or s.lower() in (\"no_data\", \"nan\", \"none\"):\n",
    "            return None\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            try:\n",
    "                cleaned = s.replace(\"np.float64\", \"\").replace(\"np.float32\", \"\")\n",
    "                cleaned = cleaned.replace(\"numpy.float64\", \"\").replace(\"numpy.float32\", \"\")\n",
    "                return ast.literal_eval(cleaned)\n",
    "            except Exception:\n",
    "                import re\n",
    "                nums = re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", s)\n",
    "                if nums:\n",
    "                    return tuple(float(x) for x in nums)\n",
    "                return None\n",
    "    return val\n",
    "\n",
    "def load_fit_table(path: str, key_cols: list) -> dict:\n",
    "    df = pd.read_csv(path)\n",
    "    out = {}\n",
    "    for _, r in df.iterrows():\n",
    "        key = tuple(r[c] for c in key_cols)\n",
    "        out[key] = r.to_dict()\n",
    "    return out\n",
    "\n",
    "\n",
    "def sample_from_dist(dist_name: str, params, u: float) -> float:\n",
    "    if params is None:\n",
    "        return 0.0\n",
    "    if isinstance(params, (list, tuple)) and len(params) == 1 and isinstance(params[0], (list, tuple)):\n",
    "        params = params[0]\n",
    "    dist = getattr(stats, dist_name)\n",
    "    return float(dist.ppf(u, *params))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RecallableBucket:\n",
    "    created_q: int\n",
    "    expiry_q: int\n",
    "    amount_remaining: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RecallableLedger:\n",
    "    rho: float\n",
    "    expiry_quarters: int\n",
    "    commitment: float\n",
    "    buckets: list = field(default_factory=list)\n",
    "\n",
    "    def _rc_cap(self) -> float:\n",
    "        return max(float(self.rho), 0.0) * max(float(self.commitment), 0.0)\n",
    "\n",
    "    def drop_expired(self, q: int) -> None:\n",
    "        if int(self.expiry_quarters) <= 0:\n",
    "            self.buckets = []\n",
    "            return\n",
    "        self.buckets = [b for b in self.buckets if b.expiry_q >= q and b.amount_remaining > 0]\n",
    "\n",
    "    def available(self, q: int) -> float:\n",
    "        self.drop_expired(q)\n",
    "        return float(sum(b.amount_remaining for b in self.buckets))\n",
    "\n",
    "    def add_recallable(self, q: int, rc_amount: float, enforce_cap: bool = True) -> float:\n",
    "        self.drop_expired(q)\n",
    "        x = max(float(rc_amount or 0.0), 0.0)\n",
    "        if x <= 0.0 or int(self.expiry_quarters) <= 0:\n",
    "            return 0.0\n",
    "        add_amt = x\n",
    "        if enforce_cap:\n",
    "            cap = self._rc_cap()\n",
    "            cur = self.available(q)\n",
    "            room = max(cap - cur, 0.0)\n",
    "            add_amt = min(add_amt, room)\n",
    "        if add_amt <= 0.0:\n",
    "            return 0.0\n",
    "        self.buckets.append(RecallableBucket(created_q=q, expiry_q=q + int(self.expiry_quarters), amount_remaining=float(add_amt)))\n",
    "        return float(add_amt)\n",
    "\n",
    "    def consume_for_drawdown(self, q: int, draw_amount: float) -> dict:\n",
    "        self.drop_expired(q)\n",
    "        need = max(float(draw_amount or 0.0), 0.0)\n",
    "        if need <= 0.0:\n",
    "            return {\"use_rc\": 0.0, \"use_commitment\": 0.0}\n",
    "        self.buckets.sort(key=lambda b: b.created_q)\n",
    "        use_rc = 0.0\n",
    "        for b in self.buckets:\n",
    "            if need <= 0:\n",
    "                break\n",
    "            take = min(b.amount_remaining, need)\n",
    "            b.amount_remaining -= take\n",
    "            need -= take\n",
    "            use_rc += take\n",
    "        return {\"use_rc\": use_rc, \"use_commitment\": max(draw_amount - use_rc, 0.0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233fdf36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T22:16:57.097273Z",
     "iopub.status.busy": "2026-02-01T22:16:57.096924Z",
     "iopub.status.idle": "2026-02-01T22:16:57.112877Z",
     "shell.execute_reply": "2026-02-01T22:16:57.111804Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- MSCI projection (from msci_projection.ipynb) ---\n",
    "\n",
    "def load_msci_quarterly(msci_xlsx_path: str) -> pd.DataFrame:\n",
    "    msci = pd.read_excel(msci_xlsx_path)\n",
    "    if \"Date\" not in msci.columns or \"SCXP Index\" not in msci.columns:\n",
    "        raise ValueError(\"MSCI file must contain columns: 'Date' and 'SCXP Index'\")\n",
    "    msci = msci[[\"Date\", \"SCXP Index\"]].copy()\n",
    "    msci[\"Date\"] = pd.to_datetime(msci[\"Date\"], errors=\"coerce\")\n",
    "    msci[\"SCXP Index\"] = pd.to_numeric(msci[\"SCXP Index\"], errors=\"coerce\")\n",
    "    msci = msci.dropna(subset=[\"Date\", \"SCXP Index\"]).sort_values(\"Date\")\n",
    "    msci[\"quarter_end\"] = msci[\"Date\"].dt.to_period(\"Q\").dt.to_timestamp(\"Q\")\n",
    "    q = (msci.groupby(\"quarter_end\", as_index=False)[\"SCXP Index\"]\n",
    "         .last().rename(columns={\"SCXP Index\": \"index_level\"})\n",
    "         .sort_values(\"quarter_end\").reset_index(drop=True))\n",
    "    q[\"msci_ret_q\"] = q[\"index_level\"].pct_change()\n",
    "    q = q.dropna(subset=[\"msci_ret_q\"]).reset_index(drop=True)\n",
    "    return q\n",
    "\n",
    "\n",
    "def label_regimes_by_quantiles(q_returns: pd.Series, low_q=0.33, high_q=0.67) -> pd.Series:\n",
    "    q_low = q_returns.quantile(low_q)\n",
    "    q_high = q_returns.quantile(high_q)\n",
    "    regime = pd.Series(index=q_returns.index, dtype=\"object\")\n",
    "    regime[q_returns <= q_low] = \"bear\"\n",
    "    regime[q_returns >= q_high] = \"bull\"\n",
    "    regime[(q_returns > q_low) & (q_returns < q_high)] = \"flat\"\n",
    "    return regime\n",
    "\n",
    "\n",
    "def estimate_transition_matrix(regimes: pd.Series, states=(\"bear\", \"flat\", \"bull\"), laplace=1.0) -> pd.DataFrame:\n",
    "    states = list(states)\n",
    "    counts = pd.DataFrame(0.0, index=states, columns=states)\n",
    "    r = regimes.dropna().tolist()\n",
    "    for a, b in zip(r[:-1], r[1:]):\n",
    "        if a in states and b in states:\n",
    "            counts.loc[a, b] += 1.0\n",
    "    counts = counts + laplace\n",
    "    P = counts.div(counts.sum(axis=1), axis=0)\n",
    "    return P\n",
    "\n",
    "\n",
    "def estimate_regime_params(df_q: pd.DataFrame, states=(\"bear\", \"flat\", \"bull\")) -> pd.DataFrame:\n",
    "    out = []\n",
    "    overall_sigma = float(df_q[\"msci_ret_q\"].std(ddof=1))\n",
    "    overall_sigma = max(overall_sigma, 1e-6)\n",
    "    for s in states:\n",
    "        sub = df_q.loc[df_q[\"regime\"] == s, \"msci_ret_q\"].dropna()\n",
    "        mu = float(sub.mean()) if len(sub) else 0.0\n",
    "        sigma = float(sub.std(ddof=1)) if len(sub) > 1 else overall_sigma\n",
    "        sigma = max(sigma, 1e-6)\n",
    "        out.append((s, mu, sigma))\n",
    "    return pd.DataFrame(out, columns=[\"regime\", \"mu_q\", \"sigma_q\"]).set_index(\"regime\")\n",
    "\n",
    "\n",
    "def apply_persistence_tilt(P: pd.DataFrame, scenario: str, k: float = 1.2) -> pd.DataFrame:\n",
    "    scenario = scenario.lower().strip()\n",
    "    if scenario not in {\"bullish\", \"neutral\", \"bearish\"}:\n",
    "        raise ValueError(\"scenario must be one of: bullish, neutral, bearish\")\n",
    "    if scenario == \"neutral\":\n",
    "        return P.copy()\n",
    "    target = \"bull\" if scenario == \"bullish\" else \"bear\"\n",
    "    P2 = P.copy()\n",
    "    for s in P2.index:\n",
    "        P2.loc[s, target] *= k\n",
    "    P2.loc[target, target] *= k\n",
    "    P2 = P2.div(P2.sum(axis=1), axis=0)\n",
    "    return P2\n",
    "\n",
    "\n",
    "def simulate_markov_regimes(P: pd.DataFrame, start_state: str, n_steps: int, rng: np.random.Generator) -> list:\n",
    "    states = list(P.index)\n",
    "    if start_state not in states:\n",
    "        start_state = \"flat\" if \"flat\" in states else states[0]\n",
    "    path = [start_state]\n",
    "    for _ in range(n_steps):\n",
    "        cur = path[-1]\n",
    "        probs = P.loc[cur].values.astype(float)\n",
    "        nxt = rng.choice(states, p=probs)\n",
    "        path.append(nxt)\n",
    "    return path[1:]\n",
    "\n",
    "\n",
    "def project_msci_mc(df_q_hist, start_quarter_end, n_quarters=40, n_sims=100,\n",
    "                    low_q=0.33, high_q=0.67, laplace=1.0, seed=1234,\n",
    "                    scenario=\"neutral\", tilt_strength=1.2):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df = df_q_hist.copy().sort_values(\"quarter_end\").reset_index(drop=True)\n",
    "    df[\"regime\"] = label_regimes_by_quantiles(df[\"msci_ret_q\"], low_q=low_q, high_q=high_q)\n",
    "    P = estimate_transition_matrix(df[\"regime\"], laplace=laplace)\n",
    "    params = estimate_regime_params(df)\n",
    "    P_tilted = apply_persistence_tilt(P, scenario=scenario, k=tilt_strength)\n",
    "\n",
    "    hist_levels = df_q_hist[[\"quarter_end\", \"index_level\"]].drop_duplicates(\"quarter_end\").sort_values(\"quarter_end\")\n",
    "    if start_quarter_end not in set(hist_levels[\"quarter_end\"]):\n",
    "        prev = hist_levels.loc[hist_levels[\"quarter_end\"] < start_quarter_end]\n",
    "        if prev.empty:\n",
    "            raise ValueError(\"Start quarter is before the first msci quarter in the file.\")\n",
    "        start_quarter_end = prev[\"quarter_end\"].iloc[-1]\n",
    "    start_level = float(hist_levels.loc[hist_levels[\"quarter_end\"] == start_quarter_end, \"index_level\"].iloc[0])\n",
    "\n",
    "    df_reg = df.loc[df[\"quarter_end\"] <= start_quarter_end].dropna(subset=[\"regime\"])\n",
    "    start_regime = df_reg[\"regime\"].iloc[-1] if not df_reg.empty else \"flat\"\n",
    "\n",
    "    future_qe = pd.period_range(start=start_quarter_end.to_period(\"Q\") + 1, periods=n_quarters, freq=\"Q\").to_timestamp(\"Q\")\n",
    "\n",
    "    rows = []\n",
    "    for sim_id in range(1, n_sims + 1):\n",
    "        regime_path = simulate_markov_regimes(P_tilted, start_regime, n_quarters, rng)\n",
    "        level = start_level\n",
    "        for qe, s in zip(future_qe, regime_path):\n",
    "            mu = float(params.loc[s, \"mu_q\"])\n",
    "            sig = float(params.loc[s, \"sigma_q\"])\n",
    "            r = mu + sig * rng.standard_normal()\n",
    "            level *= (1.0 + r)\n",
    "            rows.append({\n",
    "                \"sim_id\": sim_id,\n",
    "                \"quarter_end\": qe,\n",
    "                \"regime\": s,\n",
    "                \"msci_ret_q\": r,\n",
    "                \"index_level\": level,\n",
    "                \"scenario\": scenario,\n",
    "            })\n",
    "    proj = pd.DataFrame(rows)\n",
    "    return proj, P, P_tilted, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d3fc994",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T22:16:57.115695Z",
     "iopub.status.busy": "2026-02-01T22:16:57.115338Z",
     "iopub.status.idle": "2026-02-01T22:16:57.665078Z",
     "shell.execute_reply": "2026-02-01T22:16:57.664214Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w3/y0xpfndx1099qx0947v35sxh0000gn/T/ipykernel_24075/3010766749.py:70: FutureWarning: Constructing PeriodIndex from fields is deprecated. Use PeriodIndex.from_fields instead.\n",
      "  df.loc[m, \"quarter_end\"] = pd.PeriodIndex(year=years, quarter=quarters, freq=\"Q\").to_timestamp(\"Q\")\n",
      "/var/folders/w3/y0xpfndx1099qx0947v35sxh0000gn/T/ipykernel_24075/3010766749.py:297: PerformanceWarning: Adding/subtracting object-dtype array to DatetimeArray not vectorized.\n",
      "  q_snap[\"FirstCloseDate\"] + q_snap[\"InvestPeriodYears\"].apply(lambda y: pd.DateOffset(years=int(y)))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m df = add_quarter_end(df)\n\u001b[32m      6\u001b[39m df = df.dropna(subset=[\u001b[33m\"\u001b[39m\u001b[33mFundID\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquarter_end\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df = \u001b[43mapply_current_grade\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msimulation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m start_qe = pd.Period(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTART_YEAR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mQ\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTART_QUARTER[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, freq=\u001b[33m\"\u001b[39m\u001b[33mQ\u001b[39m\u001b[33m\"\u001b[39m).to_timestamp(\u001b[33m\"\u001b[39m\u001b[33mQ\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m START_FROM_HIST_END \u001b[38;5;129;01mand\u001b[39;00m HIST_END:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 297\u001b[39m, in \u001b[36mapply_current_grade\u001b[39m\u001b[34m(df, context)\u001b[39m\n\u001b[32m    289\u001b[39m fund_timing[\u001b[33m\"\u001b[39m\u001b[33mInvestPeriodYears\u001b[39m\u001b[33m\"\u001b[39m] = fund_timing[\u001b[33m\"\u001b[39m\u001b[33mBaseYears\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[32m1\u001b[39m\n\u001b[32m    291\u001b[39m q_snap = q_snap.merge(\n\u001b[32m    292\u001b[39m     fund_timing[[\u001b[33m\"\u001b[39m\u001b[33mFundID\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mFirstCloseDate\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mInvestPeriodYears\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAdjStrategy\u001b[39m\u001b[33m\"\u001b[39m]],\n\u001b[32m    293\u001b[39m     on=[\u001b[33m\"\u001b[39m\u001b[33mFundID\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAdjStrategy\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    294\u001b[39m     how=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    295\u001b[39m )\n\u001b[32m    296\u001b[39m q_snap[\u001b[33m\"\u001b[39m\u001b[33mIsInvestmentPeriod\u001b[39m\u001b[33m\"\u001b[39m] = q_snap[\u001b[33m\"\u001b[39m\u001b[33mQuarterEndDate\u001b[39m\u001b[33m\"\u001b[39m] <= (\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     \u001b[43mq_snap\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFirstCloseDate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_snap\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mInvestPeriodYears\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDateOffset\u001b[49m\u001b[43m(\u001b[49m\u001b[43myears\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m )\n\u001b[32m    300\u001b[39m fund_counts = (\n\u001b[32m    301\u001b[39m     q_snap.groupby([\u001b[33m\"\u001b[39m\u001b[33mAdjStrategy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mQPeriod\u001b[39m\u001b[33m\"\u001b[39m])[\u001b[33m\"\u001b[39m\u001b[33mFundID\u001b[39m\u001b[33m\"\u001b[39m].nunique().rename(\u001b[33m\"\u001b[39m\u001b[33mStrategyFundCount\u001b[39m\u001b[33m\"\u001b[39m).reset_index()\n\u001b[32m    302\u001b[39m )\n\u001b[32m    303\u001b[39m q_snap = q_snap.merge(fund_counts, on=[\u001b[33m\"\u001b[39m\u001b[33mAdjStrategy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mQPeriod\u001b[39m\u001b[33m\"\u001b[39m], how=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/arraylike.py:186\u001b[39m, in \u001b[36mOpsMixin.__add__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__add__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__add__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m    100\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[33;03m    Get Addition of DataFrame and other, column-wise.\u001b[39;00m\n\u001b[32m    102\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    184\u001b[39m \u001b[33;03m    moose     3.0     NaN\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/series.py:6146\u001b[39m, in \u001b[36mSeries._arith_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6144\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[32m   6145\u001b[39m     \u001b[38;5;28mself\u001b[39m, other = \u001b[38;5;28mself\u001b[39m._align_for_op(other)\n\u001b[32m-> \u001b[39m\u001b[32m6146\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/base.py:1391\u001b[39m, in \u001b[36mIndexOpsMixin._arith_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   1388\u001b[39m     rvalues = np.arange(rvalues.start, rvalues.stop, rvalues.step)\n\u001b[32m   1390\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m np.errstate(\u001b[38;5;28mall\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1391\u001b[39m     result = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43marithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(result, name=res_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:273\u001b[39m, in \u001b[36marithmetic_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# NB: We assume that extract_array and ensure_wrapped_if_datetimelike\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[38;5;66;03m#  have already been called on `left` and `right`,\u001b[39;00m\n\u001b[32m    262\u001b[39m \u001b[38;5;66;03m#  and `maybe_prepare_scalar_for_op` has already been called on `right`\u001b[39;00m\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# We need to special-case datetime64/timedelta64 dtypes (e.g. because numpy\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# casts integer dtypes to timedelta64 when operating with timedelta64 - GH#22390)\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    267\u001b[39m     should_extension_dispatch(left, right)\n\u001b[32m    268\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(right, (Timedelta, BaseOffset, Timestamp))\n\u001b[32m   (...)\u001b[39m\u001b[32m    271\u001b[39m     \u001b[38;5;66;03m# Timedelta/Timestamp and other custom scalars are included in the check\u001b[39;00m\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# because numexpr will fail on it, see GH#31457\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     res_values = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# TODO we should handle EAs consistently and move this check before the if/else\u001b[39;00m\n\u001b[32m    276\u001b[39m     \u001b[38;5;66;03m# (https://github.com/pandas-dev/pandas/issues/41165)\u001b[39;00m\n\u001b[32m    277\u001b[39m     \u001b[38;5;66;03m# error: Argument 2 to \"_bool_arith_check\" has incompatible type\u001b[39;00m\n\u001b[32m    278\u001b[39m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[32m    279\u001b[39m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/arrays/datetimelike.py:1416\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin.__add__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m   1413\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._add_timedelta_arraylike(other)\n\u001b[32m   1414\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_object_dtype(other_dtype):\n\u001b[32m   1415\u001b[39m     \u001b[38;5;66;03m# e.g. Array/Index of DateOffset objects\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1416\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_addsub_object_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1417\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m lib.is_np_dtype(other_dtype, \u001b[33m\"\u001b[39m\u001b[33mM\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m   1418\u001b[39m     other_dtype, DatetimeTZDtype\n\u001b[32m   1419\u001b[39m ):\n\u001b[32m   1420\u001b[39m     \u001b[38;5;66;03m# DatetimeIndex, ndarray[datetime64]\u001b[39;00m\n\u001b[32m   1421\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._add_datetime_arraylike(other)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/arrays/datetimelike.py:1373\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin._addsub_object_array\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   1370\u001b[39m \u001b[38;5;66;03m# Caller is responsible for broadcasting if necessary\u001b[39;00m\n\u001b[32m   1371\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.shape == other.shape, (\u001b[38;5;28mself\u001b[39m.shape, other.shape)\n\u001b[32m-> \u001b[39m\u001b[32m1373\u001b[39m res_values = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mO\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/offsets.pyx:468\u001b[39m, in \u001b[36mpandas._libs.tslibs.offsets.BaseOffset.__radd__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/offsets.pyx:463\u001b[39m, in \u001b[36mpandas._libs.tslibs.offsets.BaseOffset.__add__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/offsets.pyx:141\u001b[39m, in \u001b[36mpandas._libs.tslibs.offsets.apply_wraps.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/offsets.pyx:1432\u001b[39m, in \u001b[36mpandas._libs.tslibs.offsets.RelativeDeltaOffset._apply\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/dateutil/relativedelta.py:405\u001b[39m, in \u001b[36mrelativedelta.__radd__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__radd__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__add__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/dateutil/relativedelta.py:377\u001b[39m, in \u001b[36mrelativedelta.__add__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    375\u001b[39m         year -= \u001b[32m1\u001b[39m\n\u001b[32m    376\u001b[39m         month += \u001b[32m12\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m day = \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcalendar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmonthrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m          \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mday\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mday\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m repl = {\u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m: year, \u001b[33m\"\u001b[39m\u001b[33mmonth\u001b[39m\u001b[33m\"\u001b[39m: month, \u001b[33m\"\u001b[39m\u001b[33mday\u001b[39m\u001b[33m\"\u001b[39m: day}\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mhour\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mminute\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msecond\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmicrosecond\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Load data + fits ---\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH, engine=\"python\")\n",
    "df = normalize_columns(df)\n",
    "df = add_quarter_end(df)\n",
    "df = df.dropna(subset=[\"FundID\", \"quarter_end\"])\n",
    "df = apply_current_grade(df, context=\"simulation\")\n",
    "\n",
    "start_qe = pd.Period(f\"{START_YEAR}Q{START_QUARTER[-1]}\", freq=\"Q\").to_timestamp(\"Q\")\n",
    "if START_FROM_HIST_END and HIST_END:\n",
    "    try:\n",
    "        y = int(HIST_END[:4]); q = int(HIST_END[-1])\n",
    "        start_qe = pd.Period(f\"{y}Q{q}\", freq=\"Q\").to_timestamp(\"Q\")\n",
    "        print(f\"Using HIST_END as start_qe: {start_qe.date()}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "quarters = [start_qe + pd.offsets.QuarterEnd(i) for i in range(1, HORIZON_Q + 1)]\n",
    "\n",
    "\n",
    "\n",
    "def pick_fit_file(selected_name: str, fallback_name: str) -> Path:\n",
    "    sel = Path(FIT_DIR) / selected_name\n",
    "    if sel.exists():\n",
    "        return sel\n",
    "    fb = Path(FIT_DIR) / fallback_name\n",
    "    if fb.exists():\n",
    "        print(f\"Warning: {selected_name} not found; using {fallback_name}.\")\n",
    "        return fb\n",
    "    raise FileNotFoundError(f\"Missing both {selected_name} and {fallback_name} in {FIT_DIR}\")\n",
    "\n",
    "ratio_sel = load_fit_table(Path(FIT_DIR) / \"ratio_fit_selected.csv\", [\"Adj Strategy\", \"Grade\", \"AgeBucket\", \"ratio\"])\n",
    "timing_sel = load_fit_table(Path(FIT_DIR) / \"timing_probs_selected.csv\", [\"Adj Strategy\", \"Grade\", \"AgeBucket\"])\n",
    "\n",
    "omega_sel = None\n",
    "omega_sel_path = Path(FIT_DIR) / \"omega_selected.csv\"\n",
    "if omega_sel_path.exists():\n",
    "    omega_sel = load_fit_table(omega_sel_path, [\"Adj Strategy\", \"Grade\", \"AgeBucket\"])\n",
    "    print(\"Loaded omega_selected.csv\")\n",
    "\n",
    "rho_event = 0.25\n",
    "rho_size = 0.15\n",
    "if Path(COPULA_PATH).exists():\n",
    "    cop = json.loads(Path(COPULA_PATH).read_text())\n",
    "    rho_event = float(cop.get(\"rho_event\", rho_event))\n",
    "    rho_size = float(cop.get(\"rho_size\", rho_size))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- NAV anchor config (data-driven) ---\n",
    "NAV_ANCHOR_ENABLED = os.environ.get(\"NAV_ANCHOR_ENABLED\", \"1\").lower() in (\"1\",\"true\",\"yes\",\"y\")\n",
    "NAV_ANCHOR_MIN_AGE_BUCKET = os.environ.get(\"NAV_ANCHOR_MIN_AGE_BUCKET\", \"12-15\")\n",
    "NAV_ANCHOR_END_Q = int(os.environ.get(\"NAV_ANCHOR_END_Q\", \"12\"))\n",
    "NAV_ANCHOR_LAMBDA_DEFAULT = float(os.environ.get(\"NAV_ANCHOR_LAMBDA\", \"0.05\"))\n",
    "NAV_ANCHOR_END_LAMBDA_DEFAULT = float(os.environ.get(\"NAV_ANCHOR_END_LAMBDA\", \"0.1\"))\n",
    "NAV_ANCHOR_MIN_MULT = float(os.environ.get(\"NAV_ANCHOR_MIN_MULT\", \"0.8\"))\n",
    "NAV_ANCHOR_MAX_MULT = float(os.environ.get(\"NAV_ANCHOR_MAX_MULT\", \"1.2\"))\n",
    "NAV_LIQUIDATE_Q_DEFAULT = int(os.environ.get(\"NAV_LIQUIDATE_Q\", \"4\"))\n",
    "\n",
    "NAV_TARGETS = {}\n",
    "NAV_TARGETS_STRAT = {}\n",
    "NAV_LAM_STRAT = {}\n",
    "NAV_LAM_END_STRAT = {}\n",
    "NAV_LIQ_Q_STRAT = {}\n",
    "\n",
    "nav_target_path = Path(FIT_DIR) / \"nav_anchor_targets.csv\"\n",
    "if nav_target_path.exists():\n",
    "    nt = pd.read_csv(nav_target_path)\n",
    "    if \"avg_nav_to_paidin\" in nt.columns:\n",
    "        for _, r in nt.iterrows():\n",
    "            s = r.get(\"Adj Strategy\")\n",
    "            a = r.get(\"AgeBucket\")\n",
    "            v = r.get(\"avg_nav_to_paidin\")\n",
    "            if pd.notna(s) and pd.notna(a) and pd.notna(v):\n",
    "                NAV_TARGETS[(s, a)] = float(v)\n",
    "        # strategy-level fallback\n",
    "        for s, g in nt.groupby(\"Adj Strategy\"):\n",
    "            v = g[\"avg_nav_to_paidin\"].mean()\n",
    "            if pd.notna(v):\n",
    "                NAV_TARGETS_STRAT[s] = float(v)\n",
    "        print(\"Loaded NAV targets from\", nav_target_path)\n",
    "\n",
    "nav_cal_path = Path(FIT_DIR) / \"nav_anchor_calibration.csv\"\n",
    "if nav_cal_path.exists():\n",
    "    nav_cal = pd.read_csv(nav_cal_path)\n",
    "    for _, r in nav_cal.iterrows():\n",
    "        s = r.get(\"Adj Strategy\")\n",
    "        if pd.isna(s):\n",
    "            continue\n",
    "        lam = r.get(\"lambda\")\n",
    "        lam_end = r.get(\"lambda_end\")\n",
    "        liq = r.get(\"liq_q_p90\") if \"liq_q_p90\" in r else r.get(\"liq_q_p75\", r.get(\"liq_q_median\"))\n",
    "        if pd.notna(lam):\n",
    "            NAV_LAM_STRAT[s] = float(lam)\n",
    "        if pd.notna(lam_end):\n",
    "            NAV_LAM_END_STRAT[s] = float(lam_end)\n",
    "        if pd.notna(liq):\n",
    "            NAV_LIQ_Q_STRAT[s] = int(liq)\n",
    "    print(\"Loaded NAV anchor calibration from\", nav_cal_path)\n",
    "\n",
    "# MSCI quarterly returns\n",
    "msci_q = None\n",
    "if Path(MSCI_PATH).exists():\n",
    "    msci_q = load_msci_quarterly(MSCI_PATH)\n",
    "    msci_q[\"msci_ret_q_lag1\"] = msci_q[\"msci_ret_q\"].shift(1)\n",
    "if msci_q is None or len(msci_q) == 0:\n",
    "    raise FileNotFoundError(\"MSCI data not loaded. Check MSCI_PATH and file contents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c58e872",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T22:16:57.667840Z",
     "iopub.status.busy": "2026-02-01T22:16:57.667473Z",
     "iopub.status.idle": "2026-02-01T22:16:57.917666Z",
     "shell.execute_reply": "2026-02-01T22:16:57.916876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTO_PACE_ONEPASS scale skipped: insufficient data\n",
      "Adjusted horizon to planned end: 65 quarters, end=2041-12-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w3/y0xpfndx1099qx0947v35sxh0000gn/T/ipykernel_25034/1094924185.py:43: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  pdraw_sga = hist_sorted.groupby(G_SGA)[\"draw_flag\"].agg([\"mean\", \"count\"])\n",
      "/var/folders/w3/y0xpfndx1099qx0947v35sxh0000gn/T/ipykernel_25034/1094924185.py:44: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  pdraw_sa = hist_sorted.groupby(G_SA)[\"draw_flag\"].agg([\"mean\", \"count\"])\n",
      "/var/folders/w3/y0xpfndx1099qx0947v35sxh0000gn/T/ipykernel_25034/1094924185.py:56: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  pace_sga_all = hist_sorted.groupby(G_SGA)[\"delta_ratio\"].mean().to_dict()\n",
      "/var/folders/w3/y0xpfndx1099qx0947v35sxh0000gn/T/ipykernel_25034/1094924185.py:57: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  pace_sga_all_n = hist_sorted.groupby(G_SGA)[\"delta_ratio\"].count().to_dict()\n",
      "/var/folders/w3/y0xpfndx1099qx0947v35sxh0000gn/T/ipykernel_25034/1094924185.py:58: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  pace_sa_all = hist_sorted.groupby(G_SA)[\"delta_ratio\"].mean().to_dict()\n",
      "/var/folders/w3/y0xpfndx1099qx0947v35sxh0000gn/T/ipykernel_25034/1094924185.py:59: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  pace_sa_all_n = hist_sorted.groupby(G_SA)[\"delta_ratio\"].count().to_dict()\n",
      "/var/folders/w3/y0xpfndx1099qx0947v35sxh0000gn/T/ipykernel_25034/1094924185.py:63: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  pace_sga_pos = hist_sorted[hist_sorted[\"draw_flag\"]].groupby(G_SGA)[\"delta_ratio\"].mean().to_dict()\n",
      "/var/folders/w3/y0xpfndx1099qx0947v35sxh0000gn/T/ipykernel_25034/1094924185.py:64: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  pace_sga_pos_n = hist_sorted[hist_sorted[\"draw_flag\"]].groupby(G_SGA)[\"delta_ratio\"].count().to_dict()\n",
      "/var/folders/w3/y0xpfndx1099qx0947v35sxh0000gn/T/ipykernel_25034/1094924185.py:65: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  pace_sa_pos = hist_sorted[hist_sorted[\"draw_flag\"]].groupby(G_SA)[\"delta_ratio\"].mean().to_dict()\n",
      "/var/folders/w3/y0xpfndx1099qx0947v35sxh0000gn/T/ipykernel_25034/1094924185.py:66: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  pace_sa_pos_n = hist_sorted[hist_sorted[\"draw_flag\"]].groupby(G_SA)[\"delta_ratio\"].count().to_dict()\n"
     ]
    }
   ],
   "source": [
    "# --- Build fund states ---\n",
    "\n",
    "hist = df[df[\"quarter_end\"] <= start_qe].copy()\n",
    "last = hist.sort_values([\"FundID\", \"quarter_end\"]).groupby(\"FundID\").tail(1)\n",
    "\n",
    "# Precompute historical cashflows for IRR and cumulative stats\n",
    "hist_sorted = hist.sort_values([\"FundID\", \"quarter_end\"]).copy()\n",
    "\n",
    "# --- Data-driven draw pace + p_draw calibration (hierarchical) ---\n",
    "AGE_BINS_Q = [-1, 3, 7, 11, 15, 19, 1000]\n",
    "AGE_LABELS = [\"0-3\", \"4-7\", \"8-11\", \"12-15\", \"16-19\", \"20+\"]\n",
    "\n",
    "hist_sorted[\"AgeBucket\"] = pd.cut(\n",
    "    pd.to_numeric(hist_sorted.get(\"Fund_Age_Quarters\"), errors=\"coerce\"),\n",
    "    bins=AGE_BINS_Q,\n",
    "    labels=AGE_LABELS,\n",
    ")\n",
    "\n",
    "# fund-level commitment map\n",
    "comm_h = pd.to_numeric(hist_sorted.get(\"Commitment EUR\"), errors=\"coerce\")\n",
    "commit_map_h = comm_h.groupby(hist_sorted[\"FundID\"]).max()\n",
    "\n",
    "# cumulative recallables for denom\n",
    "rc_cum_h = hist_sorted.groupby(\"FundID\")[\"Recallable\"].transform(\n",
    "    lambda s: pd.to_numeric(s, errors=\"coerce\").abs().cumsum()\n",
    ")\n",
    "\n",
    "# denom for cumulative draw ratio\n",
    "denom_h = hist_sorted[\"FundID\"].map(commit_map_h).fillna(0.0) + rc_cum_h.fillna(0.0)\n",
    "\n",
    "# draw flags + delta ratio\n",
    "delta_draw = pd.to_numeric(hist_sorted.get(\"Adj Drawdown EUR\"), errors=\"coerce\").abs().fillna(0.0)\n",
    "hist_sorted[\"draw_flag\"] = delta_draw > 0\n",
    "delta_ratio = np.where(denom_h > 0, delta_draw / denom_h, 0.0)\n",
    "hist_sorted[\"delta_ratio\"] = delta_ratio\n",
    "\n",
    "# group keys\n",
    "G_SGA = [\"Adj Strategy\", \"Grade\", \"AgeBucket\"]\n",
    "G_SA = [\"Adj Strategy\", \"AgeBucket\"]\n",
    "G_S = [\"Adj Strategy\"]\n",
    "\n",
    "# p_draw by group with counts\n",
    "pdraw_sga = hist_sorted.groupby(G_SGA)[\"draw_flag\"].agg([\"mean\", \"count\"])\n",
    "pdraw_sa = hist_sorted.groupby(G_SA)[\"draw_flag\"].agg([\"mean\", \"count\"])\n",
    "pdraw_s = hist_sorted.groupby(G_S)[\"draw_flag\"].agg([\"mean\", \"count\"])\n",
    "\n",
    "p_draw_sga = pdraw_sga[\"mean\"].to_dict()\n",
    "p_draw_sga_n = pdraw_sga[\"count\"].to_dict()\n",
    "p_draw_sa = pdraw_sa[\"mean\"].to_dict()\n",
    "p_draw_sa_n = pdraw_sa[\"count\"].to_dict()\n",
    "p_draw_s = pdraw_s[\"mean\"].to_dict()\n",
    "p_draw_s_n = pdraw_s[\"count\"].to_dict()\n",
    "p_draw_global = float(hist_sorted[\"draw_flag\"].mean()) if len(hist_sorted) else 0.0\n",
    "\n",
    "# draw pace by group (delta ratio)\n",
    "pace_sga_all = hist_sorted.groupby(G_SGA)[\"delta_ratio\"].mean().to_dict()\n",
    "pace_sga_all_n = hist_sorted.groupby(G_SGA)[\"delta_ratio\"].count().to_dict()\n",
    "pace_sa_all = hist_sorted.groupby(G_SA)[\"delta_ratio\"].mean().to_dict()\n",
    "pace_sa_all_n = hist_sorted.groupby(G_SA)[\"delta_ratio\"].count().to_dict()\n",
    "pace_s_all = hist_sorted.groupby(G_S)[\"delta_ratio\"].mean().to_dict()\n",
    "pace_s_all_n = hist_sorted.groupby(G_S)[\"delta_ratio\"].count().to_dict()\n",
    "\n",
    "pace_sga_pos = hist_sorted[hist_sorted[\"draw_flag\"]].groupby(G_SGA)[\"delta_ratio\"].mean().to_dict()\n",
    "pace_sga_pos_n = hist_sorted[hist_sorted[\"draw_flag\"]].groupby(G_SGA)[\"delta_ratio\"].count().to_dict()\n",
    "pace_sa_pos = hist_sorted[hist_sorted[\"draw_flag\"]].groupby(G_SA)[\"delta_ratio\"].mean().to_dict()\n",
    "pace_sa_pos_n = hist_sorted[hist_sorted[\"draw_flag\"]].groupby(G_SA)[\"delta_ratio\"].count().to_dict()\n",
    "pace_s_pos = hist_sorted[hist_sorted[\"draw_flag\"]].groupby(G_S)[\"delta_ratio\"].mean().to_dict()\n",
    "pace_s_pos_n = hist_sorted[hist_sorted[\"draw_flag\"]].groupby(G_S)[\"delta_ratio\"].count().to_dict()\n",
    "\n",
    "pace_global_all = float(hist_sorted[\"delta_ratio\"].mean()) if len(hist_sorted) else 0.0\n",
    "pace_global_pos = float(hist_sorted.loc[hist_sorted[\"draw_flag\"], \"delta_ratio\"].mean()) if hist_sorted[\"draw_flag\"].any() else 0.0\n",
    "\n",
    "# aliases for downstream compatibility\n",
    "draw_pace_global_all = pace_global_all\n",
    "draw_pace_global_pos = pace_global_pos\n",
    "\n",
    "\n",
    "# Build timing fallback maps from timing_sel\n",
    "_timing_rows = []\n",
    "for (s, g, a), v in timing_sel.items():\n",
    "    _timing_rows.append({\n",
    "        \"Adj Strategy\": s,\n",
    "        \"Grade\": g,\n",
    "        \"AgeBucket\": a,\n",
    "        \"p_draw\": float(v.get(\"p_draw\", 0.0)),\n",
    "        \"p_rep\": float(v.get(\"p_rep\", 0.0)),\n",
    "        \"p_rc_given_rep\": float(v.get(\"p_rc_given_rep\", 0.0)),\n",
    "    })\n",
    "_timing_df = pd.DataFrame(_timing_rows)\n",
    "\n",
    "if len(_timing_df):\n",
    "    timing_by_sa = _timing_df.groupby([\"Adj Strategy\", \"AgeBucket\"])[[\"p_draw\", \"p_rep\", \"p_rc_given_rep\"]].mean().to_dict(\"index\")\n",
    "    timing_by_s = _timing_df.groupby([\"Adj Strategy\"])[[\"p_draw\", \"p_rep\", \"p_rc_given_rep\"]].mean().to_dict(\"index\")\n",
    "    timing_global = _timing_df[[\"p_draw\", \"p_rep\", \"p_rc_given_rep\"]].mean().to_dict()\n",
    "else:\n",
    "    timing_by_sa = {}\n",
    "    timing_by_s = {}\n",
    "    timing_global = {\"p_draw\": 0.0, \"p_rep\": 0.0, \"p_rc_given_rep\": 0.0}\n",
    "\n",
    "P_DRAW_MIN_N = 50\n",
    "PACE_MIN_N = 50\n",
    "P_DRAW_MULT_MIN = 0.5\n",
    "P_DRAW_MULT_MAX = 2.0\n",
    "\n",
    "\n",
    "def get_timing_probs(strategy, grade, age_bucket):\n",
    "    tp = timing_sel.get((strategy, grade, age_bucket))\n",
    "    if tp is None:\n",
    "        tp = timing_by_sa.get((strategy, age_bucket))\n",
    "    if tp is None:\n",
    "        tp = timing_by_s.get(strategy)\n",
    "    if tp is None:\n",
    "        tp = timing_global\n",
    "    if tp is None:\n",
    "        return {\"p_draw\": 0.0, \"p_rep\": 0.0, \"p_rc_given_rep\": 0.0}\n",
    "    return tp\n",
    "\n",
    "\n",
    "def get_hist_p_draw_and_n(strategy, grade, age_bucket):\n",
    "    key_sga = (strategy, grade, age_bucket)\n",
    "    if key_sga in p_draw_sga and p_draw_sga_n.get(key_sga, 0) >= P_DRAW_MIN_N:\n",
    "        return float(p_draw_sga[key_sga]), int(p_draw_sga_n.get(key_sga, 0))\n",
    "    key_sa = (strategy, age_bucket)\n",
    "    if key_sa in p_draw_sa and p_draw_sa_n.get(key_sa, 0) >= P_DRAW_MIN_N:\n",
    "        return float(p_draw_sa[key_sa]), int(p_draw_sa_n.get(key_sa, 0))\n",
    "    if strategy in p_draw_s and p_draw_s_n.get(strategy, 0) >= P_DRAW_MIN_N:\n",
    "        return float(p_draw_s[strategy]), int(p_draw_s_n.get(strategy, 0))\n",
    "    return float(p_draw_global), int(len(hist_sorted))\n",
    "\n",
    "\n",
    "def get_model_p_draw(strategy, grade, age_bucket):\n",
    "    tp = get_timing_probs(strategy, grade, age_bucket)\n",
    "    return float(tp.get(\"p_draw\", 0.0))\n",
    "\n",
    "\n",
    "def get_p_draw_multiplier(strategy, grade, age_bucket):\n",
    "    hist_val, _n = get_hist_p_draw_and_n(strategy, grade, age_bucket)\n",
    "    model_val = get_model_p_draw(strategy, grade, age_bucket)\n",
    "    if model_val <= 0 or not np.isfinite(model_val):\n",
    "        return 1.0\n",
    "    mult = hist_val / model_val if np.isfinite(hist_val) else 1.0\n",
    "    if not np.isfinite(mult):\n",
    "        mult = 1.0\n",
    "    return float(np.clip(mult, P_DRAW_MULT_MIN, P_DRAW_MULT_MAX))\n",
    "\n",
    "\n",
    "def get_blend_weight(n_obs: int) -> float:\n",
    "    if n_obs >= 200:\n",
    "        return 0.7\n",
    "    if n_obs >= 100:\n",
    "        return 0.5\n",
    "    if n_obs >= 50:\n",
    "        return 0.35\n",
    "    return 0.2\n",
    "\n",
    "\n",
    "def get_p_draw_adjusted(strategy, grade, age_bucket):\n",
    "    hist_val, n_obs = get_hist_p_draw_and_n(strategy, grade, age_bucket)\n",
    "    model_val = get_model_p_draw(strategy, grade, age_bucket)\n",
    "    mult = get_p_draw_multiplier(strategy, grade, age_bucket)\n",
    "    p_mult = model_val * mult\n",
    "    w = get_blend_weight(n_obs)\n",
    "    if not np.isfinite(hist_val):\n",
    "        p_adj = p_mult\n",
    "    elif not np.isfinite(p_mult):\n",
    "        p_adj = hist_val\n",
    "    else:\n",
    "        p_adj = (1.0 - w) * p_mult + w * hist_val\n",
    "    # uplift-only: never below model\n",
    "    p_adj = max(p_adj, model_val)\n",
    "    return float(np.clip(p_adj, 0.0, 0.95))\n",
    "\n",
    "\n",
    "def get_draw_pace(strategy, grade, age_bucket, use_positive=False):\n",
    "    if use_positive:\n",
    "        pace_sga = pace_sga_pos\n",
    "        pace_sga_n = pace_sga_pos_n\n",
    "        pace_sa = pace_sa_pos\n",
    "        pace_sa_n = pace_sa_pos_n\n",
    "        pace_s = pace_s_pos\n",
    "        pace_s_n = pace_s_pos_n\n",
    "        pace_global = pace_global_pos\n",
    "    else:\n",
    "        pace_sga = pace_sga_all\n",
    "        pace_sga_n = pace_sga_all_n\n",
    "        pace_sa = pace_sa_all\n",
    "        pace_sa_n = pace_sa_all_n\n",
    "        pace_s = pace_s_all\n",
    "        pace_s_n = pace_s_all_n\n",
    "        pace_global = pace_global_all\n",
    "\n",
    "    key_sga = (strategy, grade, age_bucket)\n",
    "    if key_sga in pace_sga and pace_sga_n.get(key_sga, 0) >= PACE_MIN_N:\n",
    "        return float(pace_sga[key_sga])\n",
    "    key_sa = (strategy, age_bucket)\n",
    "    if key_sa in pace_sa and pace_sa_n.get(key_sa, 0) >= PACE_MIN_N:\n",
    "        return float(pace_sa[key_sa])\n",
    "    if strategy in pace_s and pace_s_n.get(strategy, 0) >= PACE_MIN_N:\n",
    "        return float(pace_s[strategy])\n",
    "    return float(pace_global)\n",
    "\n",
    "\n",
    "\n",
    "# fund-level commitment (Commitment EUR only)\n",
    "comm = pd.to_numeric(hist_sorted.get(\"Commitment EUR\"), errors=\"coerce\")\n",
    "commitment_map = comm.groupby(hist_sorted[\"FundID\"]).max()\n",
    "flow_map = {}\n",
    "rep_cum_map = {}\n",
    "draw_cum_map = {}\n",
    "for fid, g in hist_sorted.groupby(\"FundID\", sort=False):\n",
    "    draw_abs = pd.to_numeric(g.get(\"Adj Drawdown EUR\"), errors=\"coerce\").fillna(0.0).abs()\n",
    "    rep_abs = pd.to_numeric(g.get(\"Adj Repayment EUR\"), errors=\"coerce\").fillna(0.0).abs()\n",
    "    flows = (-draw_abs + rep_abs).astype(float).tolist()\n",
    "    dates = g[\"quarter_end\"].tolist()\n",
    "    flow_map[fid] = (flows, dates)\n",
    "    draw_cum_map[fid] = float(draw_abs.sum())\n",
    "    rep_cum_map[fid] = float(rep_abs.sum())\n",
    "\n",
    "recall_cum_map = {}\n",
    "for fid, g in hist_sorted.groupby(\"FundID\", sort=False):\n",
    "    rc_abs = pd.to_numeric(g.get(\"Recallable\"), errors=\"coerce\").fillna(0.0).abs()\n",
    "    recall_cum_map[fid] = float(rc_abs.sum())\n",
    "\n",
    "# Investment period timing (5+1 if repayments start within 5y else 6+1)\n",
    "first_close = hist_sorted.groupby(\"FundID\")[\"First Closing Date\"].min()\n",
    "first_close = pd.to_datetime(first_close, errors=\"coerce\")\n",
    "if first_close.isna().any():\n",
    "    fallback = hist_sorted.groupby(\"FundID\")[\"quarter_end\"].min()\n",
    "    first_close = first_close.fillna(fallback)\n",
    "\n",
    "first_repay = hist_sorted[hist_sorted[\"Adj Repayment EUR\"].abs() > 0].groupby(\"FundID\")[\"quarter_end\"].min()\n",
    "first_repay = first_repay.reindex(first_close.index)\n",
    "repay_within_5y = first_repay.notna() & (first_repay <= (first_close + pd.DateOffset(years=5)))\n",
    "base_years = np.where(repay_within_5y, 5, 6)\n",
    "invest_years = pd.Series(base_years + 1, index=first_close.index)\n",
    "invest_end = pd.Series(index=first_close.index, dtype=\"datetime64[ns]\")\n",
    "for fid, fc in first_close.items():\n",
    "    if pd.isna(fc):\n",
    "        invest_end.loc[fid] = pd.NaT\n",
    "    else:\n",
    "        invest_end.loc[fid] = fc + pd.DateOffset(years=int(invest_years.loc[fid]))\n",
    "\n",
    "\n",
    "# Planned end + strategy overrun (from history)\n",
    "planned_end = hist_sorted.groupby(\"FundID\")[\"Planned End Date\"].last() if \"Planned End Date\" in hist_sorted.columns else pd.Series(index=first_close.index, dtype=\"datetime64[ns]\")\n",
    "planned_end = pd.to_datetime(planned_end, errors=\"coerce\")\n",
    "planned_end_qe = planned_end.dt.to_period(\"Q\").dt.to_timestamp(\"Q\")\n",
    "last_obs = hist_sorted.groupby(\"FundID\")[\"quarter_end\"].max()\n",
    "\n",
    "def quarters_diff(a, b):\n",
    "    if pd.isna(a) or pd.isna(b):\n",
    "        return np.nan\n",
    "    return float(pd.Period(a, freq=\"Q\").ordinal - pd.Period(b, freq=\"Q\").ordinal)\n",
    "\n",
    "overrun_q = (last_obs.to_frame(\"last_qe\").join(planned_end_qe.rename(\"planned_end_qe\"))\n",
    "            .apply(lambda r: max(quarters_diff(r[\"last_qe\"], r[\"planned_end_qe\"]), 0.0) if pd.notna(r[\"planned_end_qe\"]) else np.nan, axis=1))\n",
    "overran_only = overrun_q[overrun_q.notna() & (overrun_q > 0)]\n",
    "fund_strategy = hist_sorted.groupby(\"FundID\")[\"Adj Strategy\"].agg(lambda s: s.mode().iat[0] if len(s.mode()) else s.iloc[0])\n",
    "avg_overrun_by_strategy = (overran_only.to_frame(\"overrun_q\")\n",
    "                           .join(fund_strategy.rename(\"Adj Strategy\"))\n",
    "                           .groupby(\"Adj Strategy\")[\"overrun_q\"].mean().clip(lower=0.0))\n",
    "\n",
    "fund_end_qe = planned_end_qe.copy()\n",
    "for fid, pe in planned_end_qe.items():\n",
    "    if pd.isna(pe):\n",
    "        continue\n",
    "    strat = fund_strategy.get(fid, None)\n",
    "    avg_over = float(avg_overrun_by_strategy.get(strat, 0.0)) if strat is not None else 0.0\n",
    "    if avg_over > 0:\n",
    "        fund_end_qe.loc[fid] = (pd.Period(pe, freq=\"Q\") + int(round(avg_over))).to_timestamp(\"Q\")\n",
    "\n",
    "fund_states = {}\n",
    "for _, r in last.iterrows():\n",
    "    fid = r[\"FundID\"]\n",
    "    strategy = r.get(\"Adj Strategy\", \"Unknown\")\n",
    "    grade = r.get(\"Grade\", \"D\")\n",
    "    status = str(r.get(\"Fund Workflow Stage\", \"\")).strip().lower()\n",
    "    if \"terminated\" in status:\n",
    "        continue\n",
    "    age_q = int(pd.to_numeric(r.get(\"Fund_Age_Quarters\", 0), errors=\"coerce\") or 0)\n",
    "    nav = float(pd.to_numeric(r.get(\"NAV Adjusted EUR\", 0), errors=\"coerce\") or 0.0)\n",
    "    fc = first_close.get(fid, pd.NaT)\n",
    "    if pd.notna(fc):\n",
    "        fc_qe = pd.Period(fc, freq=\"Q\").to_timestamp(\"Q\")\n",
    "        start_qe_fund = max(fc_qe, start_qe)\n",
    "    else:\n",
    "        start_qe_fund = start_qe\n",
    "    commitment = float(commitment_map.get(fid, 0.0) or 0.0)\n",
    "    dd_commit = float(pd.to_numeric(r.get(\"draw_cum_prev\", 0), errors=\"coerce\") or 0.0)\n",
    "    rho = float(pd.to_numeric(r.get(\"Recallable_Percentage_Decimal\", 0), errors=\"coerce\") or 0.0)\n",
    "    exp_q_val = pd.to_numeric(r.get(\"Expiration_Quarters\", 0), errors=\"coerce\")\n",
    "    exp_q = int(exp_q_val) if pd.notna(exp_q_val) else 0\n",
    "\n",
    "    flows, dates = flow_map.get(fid, ([], []))\n",
    "\n",
    "    fund_states[fid] = {\n",
    "        \"strategy\": strategy,\n",
    "        \"grade\": grade,\n",
    "        \"grade_seed\": grade,\n",
    "        \"age0\": age_q,\n",
    "        \"start_qe\": start_qe_fund,\n",
    "        \"nav\": nav,\n",
    "        \"dd_commit\": dd_commit,\n",
    "        \"draw_cum\": draw_cum_map.get(fid, 0.0),\n",
    "        \"draw_cum_ratio\": draw_cum_map.get(fid, 0.0) / commitment if commitment else 0.0,\n",
    "        \"rep_cum\": rep_cum_map.get(fid, 0.0),\n",
    "        \"recall_cum\": recall_cum_map.get(fid, 0.0),\n",
    "        \"commitment\": commitment,\n",
    "        \"ledger\": RecallableLedger(rho=rho, expiry_quarters=exp_q, commitment=commitment),\n",
    "        \"cf_amounts\": list(flows),\n",
    "        \"cf_dates\": list(dates),\n",
    "        \"invest_end\": invest_end.get(fid, pd.NaT),\n",
    "        \"fund_end_qe\": fund_end_qe.get(fid, pd.NaT),\n",
    "    }\n",
    "\n",
    "# include funds with no historical cashflows/NAV (but active)\n",
    "all_sorted = df.sort_values([\"FundID\", \"quarter_end\"])\n",
    "all_last = all_sorted.groupby(\"FundID\").tail(1)\n",
    "# fund-level commitment map from full data\n",
    "comm_all = pd.to_numeric(all_sorted.get(\"Commitment EUR\"), errors=\"coerce\")\n",
    "commit_map_all = comm_all.groupby(all_sorted[\"FundID\"]).max()\n",
    "first_close_all = all_sorted.groupby(\"FundID\")[\"First Closing Date\"].min()\n",
    "first_close_all = pd.to_datetime(first_close_all, errors=\"coerce\")\n",
    "planned_end_all = all_sorted.groupby(\"FundID\")[\"Planned End Date\"].last() if \"Planned End Date\" in all_sorted.columns else pd.Series(index=first_close_all.index, dtype=\"datetime64[ns]\")\n",
    "planned_end_all = pd.to_datetime(planned_end_all, errors=\"coerce\")\n",
    "planned_end_qe_all = planned_end_all.dt.to_period(\"Q\").dt.to_timestamp(\"Q\")\n",
    "fund_strategy_all = all_sorted.groupby(\"FundID\")[\"Adj Strategy\"].agg(lambda s: s.mode().iat[0] if len(s.mode()) else s.iloc[0])\n",
    "# use last non-null grade\n",
    "fund_grade_all = all_sorted.groupby(\"FundID\")[\"Grade\"].apply(lambda s: s.dropna().iloc[-1] if s.dropna().shape[0] else np.nan)\n",
    "\n",
    "for _, r in all_last.iterrows():\n",
    "    fid = r[\"FundID\"]\n",
    "    if fid in fund_states:\n",
    "        continue\n",
    "    status = str(r.get(\"Fund Workflow Stage\", \"\")).strip().lower()\n",
    "    if \"terminated\" in status:\n",
    "        continue\n",
    "    fc = first_close_all.get(fid, pd.NaT)\n",
    "    if pd.isna(fc) or fc > start_qe:\n",
    "        continue\n",
    "    commitment = float(commit_map_all.get(fid, 0.0) or 0.0)\n",
    "    if commitment <= 0:\n",
    "        continue\n",
    "    strategy = fund_strategy_all.get(fid, r.get(\"Adj Strategy\", \"Unknown\"))\n",
    "    grade = fund_grade_all.get(fid, r.get(\"Grade\", \"D\"))\n",
    "    age_val = pd.to_numeric(r.get(\"Fund_Age_Quarters\", np.nan), errors=\"coerce\")\n",
    "    if pd.isna(age_val):\n",
    "        # compute age from first close to cutoff\n",
    "        age_val = max(0, int(pd.Period(start_qe, freq=\"Q\").ordinal - pd.Period(fc, freq=\"Q\").ordinal))\n",
    "    age_q = int(age_val)\n",
    "    nav = 0.0\n",
    "    dd_commit = 0.0\n",
    "    rho = float(pd.to_numeric(r.get(\"Recallable_Percentage_Decimal\", 0), errors=\"coerce\") or 0.0)\n",
    "    exp_q_val = pd.to_numeric(r.get(\"Expiration_Quarters\", 0), errors=\"coerce\")\n",
    "    exp_q = int(exp_q_val) if pd.notna(exp_q_val) else 0\n",
    "\n",
    "    # investment period for funds without repayments: default 6+1 years\n",
    "    invest_end_f = fc + pd.DateOffset(years=7) if pd.notna(fc) else pd.NaT\n",
    "\n",
    "    pe = planned_end_qe_all.get(fid, pd.NaT)\n",
    "    if pd.notna(pe):\n",
    "        strat = fund_strategy_all.get(fid, None)\n",
    "        avg_over = float(avg_overrun_by_strategy.get(strat, 0.0)) if strat is not None else 0.0\n",
    "        if avg_over > 0:\n",
    "            pe = (pd.Period(pe, freq=\"Q\") + int(round(avg_over))).to_timestamp(\"Q\")\n",
    "    fund_states[fid] = {\n",
    "        \"strategy\": strategy,\n",
    "        \"grade\": grade if pd.notna(grade) else \"D\",\n",
    "        \"age0\": age_q,\n",
    "        \"start_qe\": start_qe_fund,\n",
    "        \"nav\": nav,\n",
    "        \"dd_commit\": dd_commit,\n",
    "        \"draw_cum\": 0.0,\n",
    "        \"draw_cum_ratio\": 0.0,\n",
    "        \"rep_cum\": 0.0,\n",
    "        \"recall_cum\": 0.0,\n",
    "        \"commitment\": commitment,\n",
    "        \"ledger\": RecallableLedger(rho=rho, expiry_quarters=exp_q, commitment=commitment),\n",
    "        \"cf_amounts\": [],\n",
    "        \"cf_dates\": [],\n",
    "        \"invest_end\": invest_end_f,\n",
    "        \"fund_end_qe\": pe,\n",
    "        \"ended\": False,\n",
    "        \"grade_end\": None,\n",
    "    }\n",
    "\n",
    "# one-pass pace calibration (optional) -- match recent history draw pace\n",
    "if AUTO_PACE_ONEPASS:\n",
    "    try:\n",
    "        # historical draw per active fund over last N quarters\n",
    "        hist_agg = df.groupby(\"quarter_end\", as_index=False).agg(\n",
    "            hist_draw=(\"Adj Drawdown EUR\", lambda s: pd.to_numeric(s, errors=\"coerce\").abs().sum()),\n",
    "            active_funds=(\"FundID\", \"nunique\"),\n",
    "        ).sort_values(\"quarter_end\")\n",
    "        hist_last = hist_agg[hist_agg[\"quarter_end\"] <= start_qe].tail(PACE_CALIB_N_Q)\n",
    "        hist_draw_per_active = float((hist_last[\"hist_draw\"] / hist_last[\"active_funds\"]).mean())\n",
    "\n",
    "        # expected draw per active fund from base pace + gating at cutoff\n",
    "        exp_draws = []\n",
    "        for fid, st in fund_states.items():\n",
    "            denom = st.get(\"commitment\", 0.0) + st.get(\"recall_cum\", 0.0)\n",
    "            if denom <= 0:\n",
    "                continue\n",
    "            age_q = max(int(st.get(\"age0\", 0)), 0)\n",
    "            age_bucket = make_age_bucket(age_q)\n",
    "            strategy = st.get(\"strategy\", \"Unknown\")\n",
    "            grade = st.get(\"grade\", \"D\")\n",
    "            if age_bucket in (\"0-3\", \"4-7\"):\n",
    "                pace = get_draw_pace(strategy, grade, age_bucket, use_positive=False)\n",
    "                if pace is None or not np.isfinite(pace) or pace <= 0:\n",
    "                    pace = draw_pace_global_all if draw_pace_global_all > 0 else 0.0\n",
    "                p_draw = get_p_draw_adjusted(strategy, grade, age_bucket)\n",
    "            else:\n",
    "                pace = get_draw_pace(strategy, grade, age_bucket, use_positive=True)\n",
    "                if pace is None or not np.isfinite(pace) or pace <= 0:\n",
    "                    pace = draw_pace_global_pos if draw_pace_global_pos > 0 else 0.0\n",
    "                # scale pace upward using adjusted vs model p_draw (uplift-only)\n",
    "                model_p = get_model_p_draw(strategy, grade, age_bucket)\n",
    "                adj_p = get_p_draw_adjusted(strategy, grade, age_bucket)\n",
    "                if model_p is not None and np.isfinite(model_p) and model_p > 0:\n",
    "                    scale = adj_p / model_p if np.isfinite(adj_p) else 1.0\n",
    "                    if np.isfinite(scale) and scale > 1.0:\n",
    "                        pace = pace * scale\n",
    "                p_draw = 1.0  # gating off after early years\n",
    "            exp_draws.append(denom * pace * p_draw)\n",
    "\n",
    "        exp_draw_per_active = float(np.mean(exp_draws)) if exp_draws else np.nan\n",
    "        if np.isfinite(hist_draw_per_active) and np.isfinite(exp_draw_per_active) and exp_draw_per_active > 0:\n",
    "            raw_scale = hist_draw_per_active / exp_draw_per_active\n",
    "            PACE_SCALE = float(raw_scale)\n",
    "            print(f\"AUTO_PACE_ONEPASS scale (raw): {raw_scale:.3f}\")\n",
    "        else:\n",
    "            print(\"AUTO_PACE_ONEPASS scale skipped: insufficient data\")\n",
    "    except Exception as e:\n",
    "        print(\"AUTO_PACE_ONEPASS scale skipped:\", e)\n",
    "\n",
    "# Update horizon to max planned end + overrun\n",
    "max_end = pd.to_datetime(pd.Series([st[\"fund_end_qe\"] for st in fund_states.values()])).dropna()\n",
    "if len(max_end):\n",
    "    max_end_qe = max_end.max()\n",
    "    total_q = int(pd.Period(max_end_qe, freq=\"Q\").ordinal - pd.Period(start_qe, freq=\"Q\").ordinal)\n",
    "    if total_q > 0:\n",
    "        quarters = [start_qe + pd.offsets.QuarterEnd(i) for i in range(1, total_q + 1)]\n",
    "        HORIZON_Q = len(quarters)\n",
    "        print(f\"Adjusted horizon to planned end: {HORIZON_Q} quarters, end={max_end_qe.date()}\")\n",
    "\n",
    "fund_ids = list(fund_states.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9bea99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T22:16:57.920459Z",
     "iopub.status.busy": "2026-02-01T22:16:57.920169Z",
     "iopub.status.idle": "2026-02-01T22:16:57.927813Z",
     "shell.execute_reply": "2026-02-01T22:16:57.926881Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Omega setup ---\n",
    "\n",
    "omega_mu = 0.0\n",
    "omega_sig = 0.0\n",
    "\n",
    "if OMEGA_MODE == \"global\":\n",
    "    df2 = df.sort_values([\"FundID\", \"quarter_end\"]).copy()\n",
    "    df2[\"nav_prev\"] = df2.groupby(\"FundID\")[\"NAV Adjusted EUR\"].shift(1)\n",
    "    df2[\"flow_net\"] = pd.to_numeric(df2[\"Adj Drawdown EUR\"], errors=\"coerce\").fillna(0.0) -                       pd.to_numeric(df2[\"Adj Repayment EUR\"], errors=\"coerce\").fillna(0.0)\n",
    "    m = df2[\"nav_prev\"].abs() > 1.0\n",
    "    omega = ((df2.loc[m, \"NAV Adjusted EUR\"] - df2.loc[m, \"nav_prev\"]) - df2.loc[m, \"flow_net\"]) / df2.loc[m, \"nav_prev\"]\n",
    "    omega = omega.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if len(omega):\n",
    "        omega_mu = float(omega.mean())\n",
    "        omega_sig = float(omega.std(ddof=1))\n",
    "\n",
    "# MSCI stats for unconditional mode\n",
    "msci_mu = 0.0\n",
    "msci_sigma = 0.0\n",
    "msci_map = {}\n",
    "msci_lag_map = {}\n",
    "if msci_q is not None and len(msci_q):\n",
    "    msci_mu = float(msci_q[\"msci_ret_q\"].mean())\n",
    "    msci_sigma = float(msci_q[\"msci_ret_q\"].std(ddof=1))\n",
    "    msci_map = dict(zip(msci_q[\"quarter_end\"], msci_q[\"msci_ret_q\"]))\n",
    "    msci_lag_map = dict(zip(msci_q[\"quarter_end\"], msci_q[\"msci_ret_q_lag1\"]))\n",
    "    if not np.isfinite(msci_sigma) or msci_sigma <= 0:\n",
    "        msci_sigma = 1e-6\n",
    "\n",
    "rng = np.random.default_rng(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842aaf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T22:16:57.930786Z",
     "iopub.status.busy": "2026-02-01T22:16:57.930450Z",
     "iopub.status.idle": "2026-02-01T22:17:03.230837Z",
     "shell.execute_reply": "2026-02-01T22:17:03.229388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: model_fits/runs/test_portfolio_2025Q3/projection/sim_outputs/msci_paths.csv\n",
      "Wrote: model_fits/runs/test_portfolio_2025Q3/projection/sim_outputs/sim_rep_stats_by_bucket.csv\n",
      "Wrote: model_fits/runs/test_portfolio_2025Q3/projection/sim_outputs/sim_portfolio_series.csv\n",
      "Wrote: model_fits/runs/test_portfolio_2025Q3/projection/sim_outputs/sim_fund_nav_mean.csv\n",
      "Wrote: model_fits/runs/test_portfolio_2025Q3/projection/sim_outputs/sim_fund_nav_end_mean.csv\n",
      "Wrote: model_fits/runs/test_portfolio_2025Q3/projection/sim_outputs/sim_fund_draw_mean.csv\n",
      "Wrote: model_fits/runs/test_portfolio_2025Q3/projection/sim_outputs/sim_fund_rep_mean.csv\n",
      "Wrote: model_fits/runs/test_portfolio_2025Q3/projection/sim_outputs/sim_fund_flow_end_mean.csv\n",
      "Suggested PACE_SCALE to match history: 0.00\n",
      "Wrote: model_fits/runs/test_portfolio_2025Q3/projection/sim_outputs/sim_diagnostics_summary.csv\n",
      "Wrote: model_fits/runs/test_portfolio_2025Q3/projection/sim_outputs/sim_diagnostics_by_age.csv\n",
      "Wrote: model_fits/runs/test_portfolio_2025Q3/projection/sim_outputs/draw_zero_reasons_by_quarter.csv\n",
      "Wrote: model_fits/runs/test_portfolio_2025Q3/projection/sim_outputs/draw_zero_reasons_summary.csv\n",
      "Wrote: model_fits/runs/test_portfolio_2025Q3/projection/sim_outputs/repayment_diagnostics_by_quarter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w3/y0xpfndx1099qx0947v35sxh0000gn/T/ipykernel_25034/3850470283.py:601: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  hist_p_draw = df_diag.groupby(\"AgeBucket\")[\"draw_flag\"].mean().rename(\"hist_p_draw\")\n"
     ]
    }
   ],
   "source": [
    "# --- Run simulation ---\n",
    "\n",
    "DEBT_STRATEGIES = {\"Hybrid Debt-Equity\", \"Private Debt\", \"Other Private Debt\"}\n",
    "VC_STRATEGY = \"Venture Capital\"\n",
    "\n",
    "GRADE_ORDER = [\"A\", \"B\", \"C\", \"D\"]\n",
    "grade_to_idx = {g: i for i, g in enumerate(GRADE_ORDER)}\n",
    "idx_to_grade = {i: g for g, i in grade_to_idx.items()}\n",
    "\n",
    "def quartile_to_grade(s):\n",
    "    r = s.rank(pct=True)\n",
    "    return pd.cut(r, [0, 0.25, 0.5, 0.75, 1], labels=[\"D\", \"C\", \"B\", \"A\"], include_lowest=True)\n",
    "\n",
    "def worse_grade(g1, g2):\n",
    "    if pd.isna(g1):\n",
    "        return g2\n",
    "    if pd.isna(g2):\n",
    "        return g1\n",
    "    return g1 if grade_to_idx[g1] >= grade_to_idx[g2] else g2\n",
    "\n",
    "def downgrade_one_notch(g):\n",
    "    if pd.isna(g):\n",
    "        return g\n",
    "    return idx_to_grade[min(grade_to_idx[g] + 1, 3)]\n",
    "\n",
    "def final_grade(row):\n",
    "    if SMALL_SAMPLE_RULE_ENABLED and row[\"StrategyFundCount\"] < 30:\n",
    "        return worse_grade(row[\"Grade_DPI\"], row[\"Grade_TVPI\"])\n",
    "    if row[\"IsDebt\"]:\n",
    "        return row[\"Grade_IRR\"]\n",
    "    if row[\"IsInvestmentPeriod\"]:\n",
    "        return row[\"Grade_TVPI\"] if row[\"IsVC\"] else row[\"Grade_DPI\"]\n",
    "    base = row[\"Grade_IRR\"]\n",
    "    dpi_g = row[\"Grade_DPI\"]\n",
    "    if pd.isna(base):\n",
    "        return base\n",
    "    if pd.notna(dpi_g) and (grade_to_idx[dpi_g] > grade_to_idx[base]):\n",
    "        return downgrade_one_notch(base)\n",
    "    return base\n",
    "\n",
    "\n",
    "def xnpv(rate, cfs, dts):\n",
    "    dts = np.asarray(dts, dtype=\"datetime64[ns]\")\n",
    "    cfs = np.asarray(cfs, dtype=float)\n",
    "    t0 = dts[0]\n",
    "    day_counts = (dts - t0) / np.timedelta64(1, \"D\")\n",
    "    years = day_counts / 365.0\n",
    "    return np.sum(cfs / ((1.0 + rate) ** years))\n",
    "\n",
    "\n",
    "def xirr_newton(cfs, dts, guess=0.1, max_iter=80, tol=1e-7):\n",
    "    dts = np.asarray(dts, dtype=\"datetime64[ns]\")\n",
    "    cfs = np.asarray(cfs, dtype=float)\n",
    "    rate = float(guess)\n",
    "    for _ in range(max_iter):\n",
    "        f = xnpv(rate, cfs, dts)\n",
    "        if not np.isfinite(f):\n",
    "            return np.nan\n",
    "        if abs(f) < tol:\n",
    "            return rate\n",
    "        eps = 1e-6\n",
    "        f1 = xnpv(rate + eps, cfs, dts)\n",
    "        df = (f1 - f) / eps\n",
    "        if df == 0 or not np.isfinite(df):\n",
    "            return np.nan\n",
    "        rate_new = rate - f / df\n",
    "        if rate_new <= -0.999999 or not np.isfinite(rate_new):\n",
    "            return np.nan\n",
    "        rate = rate_new\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def compute_state_irr(st, qe, tvpi):\n",
    "    if len(st[\"cf_amounts\"]) == 0:\n",
    "        return np.nan\n",
    "    cfs = np.asarray(st[\"cf_amounts\"], dtype=float)\n",
    "    dts = np.asarray(st[\"cf_dates\"], dtype=\"datetime64[ns]\")\n",
    "    terminal_nav = float(abs(st[\"nav\"])) if np.isfinite(st[\"nav\"]) else 0.0\n",
    "    cfs_full = np.append(cfs, terminal_nav)\n",
    "    dts_full = np.append(dts, np.datetime64(qe, \"ns\"))\n",
    "    if not (np.any(cfs_full < 0) and np.any(cfs_full > 0)):\n",
    "        return np.nan\n",
    "    guess = 0.10 if (pd.notna(tvpi) and tvpi > 1.0) else -0.10\n",
    "    irr = xirr_newton(cfs_full, dts_full, guess=guess)\n",
    "    if pd.notna(tvpi) and (0.98 <= tvpi <= 1.02):\n",
    "        if not np.isfinite(irr):\n",
    "            irr2 = xirr_newton(cfs_full, dts_full, guess=-guess)\n",
    "            if np.isfinite(irr2):\n",
    "                irr = irr2\n",
    "            else:\n",
    "                irr = 0.0\n",
    "    return irr\n",
    "\n",
    "\n",
    "def assign_current_grades(metrics_df):\n",
    "    dfm = metrics_df.copy()\n",
    "    counts = dfm.groupby(\"AdjStrategy\")[\"FundID\"].nunique().rename(\"StrategyFundCount\").reset_index()\n",
    "    dfm = dfm.merge(counts, on=\"AdjStrategy\", how=\"left\")\n",
    "    dfm[\"Grade_DPI\"] = dfm.groupby([\"AdjStrategy\"])[\"DPI\"].transform(quartile_to_grade)\n",
    "    dfm[\"Grade_TVPI\"] = dfm.groupby([\"AdjStrategy\"])[\"TVPI\"].transform(quartile_to_grade)\n",
    "    dfm[\"Grade_IRR\"] = dfm.groupby([\"AdjStrategy\"])[\"IRR\"].transform(quartile_to_grade)\n",
    "    dfm[\"IsDebt\"] = dfm[\"AdjStrategy\"].isin(DEBT_STRATEGIES)\n",
    "    dfm[\"IsVC\"] = dfm[\"AdjStrategy\"].eq(VC_STRATEGY)\n",
    "    dfm[\"CurrentGrade\"] = dfm.apply(final_grade, axis=1)\n",
    "    return dfm\n",
    "\n",
    "\n",
    "sim_nav = np.zeros((N_SIMS, HORIZON_Q))\n",
    "nav_sum_fund = None\n",
    "fund_index = {fid: i for i, fid in enumerate(fund_ids)}\n",
    "nav_sum_fund = np.zeros((len(fund_ids), HORIZON_Q))\n",
    "draw_sum_fund = np.zeros((len(fund_ids), HORIZON_Q))\n",
    "rep_sum_fund = np.zeros((len(fund_ids), HORIZON_Q))\n",
    "sim_draw = np.zeros((N_SIMS, HORIZON_Q))\n",
    "sim_rep = np.zeros((N_SIMS, HORIZON_Q))\n",
    "rep_bucket_stats = {}\n",
    "\n",
    "def _rep_stat_key(strategy, grade, age_bucket):\n",
    "    return (strategy, grade, age_bucket)\n",
    "\n",
    "def _rep_stat_update(key, obs_inc=0, event_inc=0, ratio_val=0.0, pos_inc=0, pos_ratio_val=0.0):\n",
    "    b = rep_bucket_stats.get(key)\n",
    "    if b is None:\n",
    "        b = {\"obs\": 0, \"events\": 0, \"sum_ratio\": 0.0, \"pos\": 0, \"sum_ratio_pos\": 0.0}\n",
    "        rep_bucket_stats[key] = b\n",
    "    b[\"obs\"] += obs_inc\n",
    "    b[\"events\"] += event_inc\n",
    "    b[\"sum_ratio\"] += ratio_val\n",
    "    b[\"pos\"] += pos_inc\n",
    "    b[\"sum_ratio_pos\"] += pos_ratio_val\n",
    "\n",
    "zero_gate = np.zeros(HORIZON_Q)\n",
    "zero_target = np.zeros(HORIZON_Q)\n",
    "zero_capacity = np.zeros(HORIZON_Q)\n",
    "zero_pace = np.zeros(HORIZON_Q)\n",
    "draw_positive = np.zeros(HORIZON_Q)\n",
    "active_count = np.zeros(HORIZON_Q)\n",
    "\n",
    "rep_event_count = np.zeros(HORIZON_Q)\n",
    "rep_navpos_count = np.zeros(HORIZON_Q)\n",
    "rep_prep_nan = np.zeros(HORIZON_Q)\n",
    "timing_key_missing = np.zeros(HORIZON_Q)\n",
    "\n",
    "rep_rr_missing = np.zeros(HORIZON_Q)\n",
    "\n",
    "\n",
    "\n",
    "# Precompute MSCI projections for unconditional mode\n",
    "proj_mc = None\n",
    "if msci_q is not None:\n",
    "    proj_mc, _, _, _ = project_msci_mc(\n",
    "        df_q_hist=msci_q,\n",
    "        start_quarter_end=start_qe,\n",
    "        n_quarters=HORIZON_Q,\n",
    "        n_sims=N_SIMS,\n",
    "        seed=SEED,\n",
    "        scenario=MSCI_SCENARIO,\n",
    "        tilt_strength=MSCI_TILT_STRENGTH,\n",
    "    )\n",
    "\n",
    "for s in range(N_SIMS):\n",
    "    # MSCI series for this simulation (projected path)\n",
    "    pm = proj_mc[proj_mc[\"sim_id\"] == (s + 1)].sort_values(\"quarter_end\")\n",
    "    msci_series = pm[\"msci_ret_q\"].tolist()\n",
    "    if len(msci_series) < HORIZON_Q:\n",
    "        msci_series = msci_series + [0.0] * (HORIZON_Q - len(msci_series))\n",
    "    msci_lag_series = [msci_series[0]] + msci_series[:-1]\n",
    "\n",
    "    state = {\n",
    "        fid: {\n",
    "            **st,\n",
    "            \"ledger\": RecallableLedger(\n",
    "                rho=st[\"ledger\"].rho,\n",
    "                expiry_quarters=st[\"ledger\"].expiry_quarters,\n",
    "                commitment=st[\"ledger\"].commitment,\n",
    "                buckets=[RecallableBucket(b.created_q, b.expiry_q, b.amount_remaining) for b in st[\"ledger\"].buckets],\n",
    "            ),\n",
    "        } for fid, st in fund_states.items()\n",
    "    }\n",
    "\n",
    "    for t, qe in enumerate(quarters):\n",
    "        U = {\n",
    "            \"draw_event\": one_factor_uniforms(len(fund_ids), rng, rho_event),\n",
    "            \"draw_size\": one_factor_uniforms(len(fund_ids), rng, rho_size),\n",
    "            \"rep_event\": one_factor_uniforms(len(fund_ids), rng, rho_event),\n",
    "            \"rep_size\": one_factor_uniforms(len(fund_ids), rng, rho_size),\n",
    "            \"rc_event\": one_factor_uniforms(len(fund_ids), rng, rho_event),\n",
    "            \"rc_size\": one_factor_uniforms(len(fund_ids), rng, rho_size),\n",
    "        }\n",
    "\n",
    "        for i, fid in enumerate(fund_ids):\n",
    "            st = state[fid]\n",
    "            if pd.notna(st.get(\"start_qe\")) and st.get(\"start_qe\") > start_qe:\n",
    "                age_q = int(pd.Period(qe, freq=\"Q\").ordinal - pd.Period(st[\"start_qe\"], freq=\"Q\").ordinal) + 1\n",
    "            else:\n",
    "                age_q = int(st[\"age0\"] + t + 1)\n",
    "            age_bucket = make_age_bucket(age_q)\n",
    "            strategy = st[\"strategy\"]\n",
    "            grade = st[\"grade\"]\n",
    "\n",
    "            fund_end = st.get(\"fund_end_qe\", pd.NaT)\n",
    "            if pd.notna(fund_end) and qe > fund_end:\n",
    "                st[\"nav\"] = 0.0\n",
    "                continue\n",
    "\n",
    "            fund_start = st.get(\"start_qe\", None)\n",
    "            if pd.notna(fund_start) and qe < fund_start:\n",
    "                continue\n",
    "\n",
    "            active_count[t] += 1\n",
    "\n",
    "            # timing probabilities (hierarchical fallback)\n",
    "            tp = get_timing_probs(strategy, grade, age_bucket)\n",
    "            if tp is None:\n",
    "                timing_key_missing[t] += 1\n",
    "            p_rep = float(tp.get(\"p_rep\", 0.0))\n",
    "            if not np.isfinite(p_rep):\n",
    "                rep_prep_nan[t] += 1\n",
    "            p_rc = float(tp.get(\"p_rc_given_rep\", 0.0))\n",
    "\n",
    "            # calibrate draw frequency to historical (multiplier + blend)\n",
    "            p_draw = get_p_draw_adjusted(strategy, grade, age_bucket)\n",
    "\n",
    "            draw_event = U[\"draw_event\"][i] < p_draw\n",
    "            # for cumulative draw ratios, relax gating after early years\n",
    "            if age_bucket not in (\"0-3\", \"4-7\"):\n",
    "                draw_event = True\n",
    "            rep_event = U[\"rep_event\"][i] < p_rep\n",
    "            if rep_event:\n",
    "                rep_event_count[t] += 1\n",
    "\n",
    "            draw_reason = None\n",
    "            if not draw_event:\n",
    "                draw_reason = \"gate\"\n",
    "\n",
    "            # draw ratio\n",
    "            rkey = (strategy, grade, age_bucket, \"draw_ratio\")\n",
    "            rr = ratio_sel.get(rkey)\n",
    "            draw_ratio = 0.0\n",
    "            if draw_event and rr is not None:\n",
    "                dist = rr.get(\"dist\")\n",
    "                params = parse_params(rr.get(\"params\"))\n",
    "                draw_ratio = sample_from_dist(dist, params, float(U[\"draw_size\"][i]))\n",
    "            if DRAW_RATIO_CAP is not None:\n",
    "                draw_ratio = float(np.clip(draw_ratio, 0.0, DRAW_RATIO_CAP))\n",
    "\n",
    "            ledger = st[\"ledger\"]\n",
    "            rc_avail = ledger.available(t)\n",
    "            remaining_commit = max(st[\"commitment\"] - st[\"dd_commit\"], 0.0)\n",
    "            capacity = remaining_commit + rc_avail\n",
    "\n",
    "            # draw_ratio now represents target cumulative draw ratio (cumulative draw / commitment)\n",
    "            target_ratio = draw_ratio\n",
    "            denom = st[\"commitment\"] + st.get(\"recall_cum\", 0.0)\n",
    "            target_cum = target_ratio * denom\n",
    "            gap_cum = max(target_cum - st[\"draw_cum\"], 0.0)\n",
    "            # smooth catch-up using data-driven draw pace by age bucket\n",
    "            if gap_cum > 0 and denom > 0:\n",
    "                # use conditional pace if draws happen every quarter (no gating), else unconditional\n",
    "                if age_bucket in (\"0-3\", \"4-7\"):\n",
    "                    pace = get_draw_pace(strategy, grade, age_bucket, use_positive=False)\n",
    "                    if pace is None or not np.isfinite(pace) or pace <= 0:\n",
    "                        pace = draw_pace_global_all if draw_pace_global_all > 0 else 0.0\n",
    "                else:\n",
    "                    pace = get_draw_pace(strategy, grade, age_bucket, use_positive=True)\n",
    "                    if pace is None or not np.isfinite(pace) or pace <= 0:\n",
    "                        pace = draw_pace_global_pos if draw_pace_global_pos > 0 else 0.0\n",
    "                    # scale pace upward using adjusted vs model p_draw (uplift-only)\n",
    "                    model_p = get_model_p_draw(strategy, grade, age_bucket)\n",
    "                    adj_p = get_p_draw_adjusted(strategy, grade, age_bucket)\n",
    "                    if model_p is not None and np.isfinite(model_p) and model_p > 0:\n",
    "                        scale = adj_p / model_p if np.isfinite(adj_p) else 1.0\n",
    "                        if np.isfinite(scale) and scale > 1.0:\n",
    "                            pace = pace * scale\n",
    "                pace = pace * PACE_SCALE\n",
    "                gap_ratio = gap_cum / denom if denom > 0 else 0.0\n",
    "                if pace > 0:\n",
    "                    exp_draw_q = max(1, int(round(gap_ratio / pace)))\n",
    "                else:\n",
    "                    exp_draw_q = 1\n",
    "                # cap to remaining quarters if fund_end_qe is known\n",
    "                if pd.notna(st.get(\"fund_end_qe\")):\n",
    "                    rem_q = int(pd.Period(st[\"fund_end_qe\"], freq=\"Q\").ordinal - pd.Period(qe, freq=\"Q\").ordinal)\n",
    "                    exp_draw_q = min(exp_draw_q, max(rem_q, 1))\n",
    "                draw_amt = min(gap_cum / exp_draw_q, gap_cum)\n",
    "            else:\n",
    "                draw_amt = 0.0\n",
    "            if draw_amt > capacity:\n",
    "                draw_amt = capacity\n",
    "            if draw_amt <= 0:\n",
    "                if draw_reason is None:\n",
    "                    if gap_cum <= 0 or denom <= 0:\n",
    "                        draw_reason = \"target\"\n",
    "                    elif capacity <= 0:\n",
    "                        draw_reason = \"capacity\"\n",
    "                    else:\n",
    "                        draw_reason = \"pace\"\n",
    "            if draw_amt > 0:\n",
    "                draw_positive[t] += 1\n",
    "            else:\n",
    "                if draw_reason == \"gate\":\n",
    "                    zero_gate[t] += 1\n",
    "                elif draw_reason == \"target\":\n",
    "                    zero_target[t] += 1\n",
    "                elif draw_reason == \"capacity\":\n",
    "                    zero_capacity[t] += 1\n",
    "                elif draw_reason == \"pace\":\n",
    "                    zero_pace[t] += 1\n",
    "            cons = ledger.consume_for_drawdown(t, draw_amt)\n",
    "            st[\"dd_commit\"] += cons[\"use_commitment\"]\n",
    "\n",
    "            # repayment ratio\n",
    "            NAV_prev = float(st[\"nav\"])\n",
    "            if NAV_prev > 1.0:\n",
    "                rep_navpos_count[t] += 1\n",
    "            rep_ratio = 0.0\n",
    "            if rep_event and NAV_prev > 1.0:\n",
    "                rkey = (strategy, grade, age_bucket, \"rep_ratio\")\n",
    "                rr = ratio_sel.get(rkey)\n",
    "                if rr is None:\n",
    "                    rep_rr_missing[t] += 1\n",
    "                if rr is not None:\n",
    "                    dist = rr.get(\"dist\")\n",
    "                    params = parse_params(rr.get(\"params\"))\n",
    "                    rep_ratio = sample_from_dist(dist, params, float(U[\"rep_size\"][i]))\n",
    "            rep_ratio = float(np.clip(rep_ratio, 0.0, 1.0))\n",
    "            # accumulate repayment stats by bucket (per-sim aggregated)\n",
    "            if NAV_prev > 1.0:\n",
    "                k = _rep_stat_key(strategy, grade, age_bucket)\n",
    "                pos = 1 if rep_ratio > 0 else 0\n",
    "                _rep_stat_update(\n",
    "                    k,\n",
    "                    obs_inc=1,\n",
    "                    event_inc=1 if rep_event else 0,\n",
    "                    ratio_val=rep_ratio,\n",
    "                    pos_inc=pos,\n",
    "                    pos_ratio_val=rep_ratio if pos else 0.0,\n",
    "                )\n",
    "\n",
    "            rep_ratio = float(np.clip(rep_ratio, 0.0, 1.0))\n",
    "            rep_amt = rep_ratio * NAV_prev\n",
    "\n",
    "            # recallable\n",
    "            rc_amt = 0.0\n",
    "            if rep_amt > 0 and (U[\"rc_event\"][i] < p_rc):\n",
    "                rkey = (strategy, grade, age_bucket, \"rc_ratio_given_rep\")\n",
    "                rr = ratio_sel.get(rkey)\n",
    "                if rr is not None:\n",
    "                    dist = rr.get(\"dist\")\n",
    "                    params = parse_params(rr.get(\"params\"))\n",
    "                    rc_ratio = sample_from_dist(dist, params, float(U[\"rc_size\"][i]))\n",
    "                    rc_ratio = float(np.clip(rc_ratio, 0.0, 1.0))\n",
    "                    rc_target = rc_ratio * rep_amt\n",
    "                    # cap recallable by cumulative repayments and cumulative drawdowns\n",
    "                    max_by_rep = max((st[\"rep_cum\"] + rep_amt) - st.get(\"recall_cum\", 0.0), 0.0)\n",
    "                    max_by_draw = max((st[\"draw_cum\"] + draw_amt) - st.get(\"recall_cum\", 0.0), 0.0)\n",
    "                    rc_target = min(rc_target, max_by_rep, max_by_draw)\n",
    "                    rc_amt = ledger.add_recallable(t, rc_target, enforce_cap=True)\n",
    "\n",
    "            # NAV update\n",
    "            nav_after_flow = max(NAV_prev + draw_amt - rep_amt, 0.0)\n",
    "            omega = 0.0\n",
    "            if OMEGA_MODE == \"global\" and omega_sig > 0:\n",
    "                omega = float(rng.normal(omega_mu, omega_sig))\n",
    "            elif OMEGA_MODE == \"calibrated\" and omega_sel is not None:\n",
    "                ok = (strategy, grade, age_bucket)\n",
    "                op = omega_sel.get(ok)\n",
    "                if op is None:\n",
    "                    op = next(iter([v for k, v in omega_sel.items() if k[0] == strategy]), None)\n",
    "                if op is not None:\n",
    "                    a0 = float(op.get(\"a_intercept\", 0.0))\n",
    "                    b0 = float(op.get(\"b0\", 0.0))\n",
    "                    b1 = float(op.get(\"b1\", 0.0))\n",
    "                    alpha = float(op.get(\"alpha\", 0.0))\n",
    "                    sigma = float(op.get(\"sigma\", 0.0))\n",
    "                    msci_r = float(msci_series[t])\n",
    "                    msci_r_lag1 = float(msci_lag_series[t])\n",
    "                    omega = (a0 + alpha) + b0 * msci_r + b1 * msci_r_lag1 + sigma * float(rng.standard_normal())\n",
    "\n",
    "            if OMEGA_CLIP is not None:\n",
    "                omega = float(np.clip(omega, OMEGA_CLIP[0], OMEGA_CLIP[1]))\n",
    "\n",
    "            nav_after = max(nav_after_flow * (1.0 + omega), 0.0)\n",
    "\n",
    "            rem_q = None\n",
    "            if pd.notna(st.get(\"fund_end_qe\")):\n",
    "                rem_q = int(pd.Period(st[\"fund_end_qe\"], freq=\"Q\").ordinal - pd.Period(qe, freq=\"Q\").ordinal) + 1\n",
    "            if NAV_ANCHOR_ENABLED:\n",
    "                paid_in_after = st[\"draw_cum\"] + draw_amt\n",
    "                if paid_in_after > 0:\n",
    "                    target = NAV_TARGETS.get((strategy, age_bucket))\n",
    "                    if target is None and NAV_TARGETS_STRAT:\n",
    "                        target = NAV_TARGETS_STRAT.get(strategy)\n",
    "                    if target is not None:\n",
    "                        age_idx = AGE_BUCKET_ORDER.get(age_bucket, -1)\n",
    "                        min_idx = AGE_BUCKET_ORDER.get(NAV_ANCHOR_MIN_AGE_BUCKET, 0)\n",
    "                        use_anchor = age_idx >= min_idx\n",
    "                        if rem_q is not None and rem_q <= NAV_ANCHOR_END_Q:\n",
    "                            use_anchor = True\n",
    "                        if use_anchor:\n",
    "                            lam = NAV_LAM_STRAT.get(strategy, NAV_ANCHOR_LAMBDA_DEFAULT)\n",
    "                            lam_end = NAV_LAM_END_STRAT.get(strategy, NAV_ANCHOR_END_LAMBDA_DEFAULT)\n",
    "                            if rem_q is not None and rem_q <= NAV_ANCHOR_END_Q:\n",
    "                                if np.isfinite(lam_end):\n",
    "                                    lam = lam_end\n",
    "                            current_ratio = nav_after / paid_in_after\n",
    "                            adj = 1.0 + lam * (target - current_ratio)\n",
    "                            adj = float(np.clip(adj, NAV_ANCHOR_MIN_MULT, NAV_ANCHOR_MAX_MULT))\n",
    "                            nav_after = max(nav_after * adj, 0.0)\n",
    "            st[\"nav\"] = nav_after\n",
    "\n",
    "            # update cumulative flows + cashflow history\n",
    "            st[\"draw_cum\"] += draw_amt\n",
    "            denom = st[\"commitment\"] + st.get(\"recall_cum\", 0.0)\n",
    "            st[\"draw_cum_ratio\"] = st[\"draw_cum\"] / denom if denom else 0.0\n",
    "            st[\"rep_cum\"] += rep_amt\n",
    "            if draw_amt != 0.0 or rep_amt != 0.0:\n",
    "                st[\"cf_amounts\"].append(-draw_amt + rep_amt)\n",
    "                st[\"cf_dates\"].append(qe)\n",
    "\n",
    "            sim_draw[s, t] += draw_amt\n",
    "            sim_rep[s, t] += rep_amt\n",
    "            sim_nav[s, t] += nav_after\n",
    "            nav_sum_fund[fund_index[fid], t] += nav_after\n",
    "            draw_sum_fund[fund_index[fid], t] += draw_amt\n",
    "            rep_sum_fund[fund_index[fid], t] += rep_amt\n",
    "        # update grades yearly using simulated performance\n",
    "        if GRADE_UPDATE_ENABLED and (t + 1) % 4 == 0:\n",
    "            metrics = []\n",
    "            for fid in fund_ids:\n",
    "                st = state[fid]\n",
    "                paid_in = float(abs(st[\"draw_cum\"]))\n",
    "                distributed = float(abs(st[\"rep_cum\"]))\n",
    "                nav = float(abs(st[\"nav\"]))\n",
    "                dpi = distributed / paid_in if paid_in > 0 else np.nan\n",
    "                tvpi = (distributed + nav) / paid_in if paid_in > 0 else np.nan\n",
    "                irr = compute_state_irr(st, qe, tvpi)\n",
    "                is_invest = False\n",
    "                if pd.notna(st.get(\"invest_end\")):\n",
    "                    is_invest = qe <= st.get(\"invest_end\")\n",
    "                metrics.append({\n",
    "                    \"FundID\": fid,\n",
    "                    \"AdjStrategy\": st[\"strategy\"],\n",
    "                    \"DPI\": dpi,\n",
    "                    \"TVPI\": tvpi,\n",
    "                    \"IRR\": irr,\n",
    "                    \"IsInvestmentPeriod\": is_invest,\n",
    "                })\n",
    "            metrics_df = pd.DataFrame(metrics)\n",
    "            metrics_df = assign_current_grades(metrics_df)\n",
    "            for row in metrics_df.itertuples(index=False):\n",
    "                st = state[row.FundID]\n",
    "                # anchor grade to first grade for first 5 years since fund start\n",
    "                if pd.notna(st.get(\"start_qe\")):\n",
    "                    q_since = int(pd.Period(qe, freq=\"Q\").ordinal - pd.Period(st[\"start_qe\"], freq=\"Q\").ordinal) + 1\n",
    "                else:\n",
    "                    q_since = int(st.get(\"age0\", 0)) + t + 1\n",
    "                if q_since < GRADE_ANCHOR_Q:\n",
    "                    if st.get(\"grade_seed\") is not None:\n",
    "                        st[\"grade\"] = str(st.get(\"grade_seed\"))\n",
    "                    continue\n",
    "                if pd.notna(row.CurrentGrade):\n",
    "                    st[\"grade\"] = str(row.CurrentGrade)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out_dir = Path(PROJ_DIR) / \"sim_outputs\"\n",
    "# write MSCI projected paths\n",
    "try:\n",
    "    if proj_mc is not None:\n",
    "        proj_mc.to_csv(out_dir / \"msci_paths.csv\", index=False)\n",
    "        print(\"Wrote:\", out_dir / \"msci_paths.csv\")\n",
    "except Exception as e:\n",
    "    print(\"MSCI path output skipped:\", e)\n",
    "\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "# write repayment stats by bucket\n",
    "try:\n",
    "    rows = []\n",
    "    for (strategy, grade, age_bucket), b in rep_bucket_stats.items():\n",
    "        obs = b.get(\"obs\", 0)\n",
    "        events = b.get(\"events\", 0)\n",
    "        pos = b.get(\"pos\", 0)\n",
    "        sum_ratio = b.get(\"sum_ratio\", 0.0)\n",
    "        sum_ratio_pos = b.get(\"sum_ratio_pos\", 0.0)\n",
    "        p_rep = (events / obs) if obs else np.nan\n",
    "        mean_uncond = (sum_ratio / obs) if obs else np.nan\n",
    "        mean_cond = (sum_ratio_pos / pos) if pos else np.nan\n",
    "        rows.append({\n",
    "            \"Adj Strategy\": strategy,\n",
    "            \"Grade\": grade,\n",
    "            \"AgeBucket\": age_bucket,\n",
    "            \"n_obs\": obs,\n",
    "            \"n_events\": events,\n",
    "            \"p_rep\": p_rep,\n",
    "            \"rep_ratio_mean_uncond\": mean_uncond,\n",
    "            \"rep_ratio_mean_cond\": mean_cond,\n",
    "        })\n",
    "    rep_stats_df = pd.DataFrame(rows)\n",
    "    rep_stats_df.to_csv(out_dir / \"sim_rep_stats_by_bucket.csv\", index=False)\n",
    "    print(\"Wrote:\", out_dir / \"sim_rep_stats_by_bucket.csv\")\n",
    "except Exception as e:\n",
    "    print(\"Rep stats output skipped:\", e)\n",
    "\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"quarter_end\": quarters,\n",
    "    \"sim_draw_mean\": sim_draw.mean(axis=0),\n",
    "    \"sim_rep_mean\": sim_rep.mean(axis=0),\n",
    "    \"sim_nav_mean\": sim_nav.mean(axis=0),\n",
    "}).to_csv(out_dir / \"sim_portfolio_series.csv\", index=False)\n",
    "\n",
    "print(\"Wrote:\", out_dir / \"sim_portfolio_series.csv\")\n",
    "# write fund-level mean NAV paths\n",
    "nav_mean = nav_sum_fund / float(N_SIMS)\n",
    "rows = []\n",
    "for i, fid in enumerate(fund_ids):\n",
    "    for t, qe in enumerate(quarters):\n",
    "        rows.append({\"FundID\": fid, \"quarter_end\": qe, \"nav_mean\": nav_mean[i, t]})\n",
    "fund_nav_path = out_dir / \"sim_fund_nav_mean.csv\"\n",
    "pd.DataFrame(rows).to_csv(fund_nav_path, index=False)\n",
    "print(\"Wrote:\", fund_nav_path)\n",
    "\n",
    "# write fund-level end-of-life NAV (mean)\n",
    "end_rows = []\n",
    "for i, fid in enumerate(fund_ids):\n",
    "    st = fund_states.get(fid, {})\n",
    "    fe = st.get(\"fund_end_qe\", None)\n",
    "    if pd.notna(fe):\n",
    "        q_idx = int(pd.Period(fe, freq=\"Q\").ordinal - pd.Period(start_qe, freq=\"Q\").ordinal)\n",
    "        q_idx = max(0, min(q_idx-1, len(quarters)-1))\n",
    "    else:\n",
    "        q_idx = len(quarters)-1\n",
    "    end_rows.append({\"FundID\": fid, \"fund_end_qe\": fe, \"nav_end_mean\": nav_mean[i, q_idx]})\n",
    "fund_nav_end = out_dir / \"sim_fund_nav_end_mean.csv\"\n",
    "pd.DataFrame(end_rows).to_csv(fund_nav_end, index=False)\n",
    "print(\"Wrote:\", fund_nav_end)\n",
    "\n",
    "# write fund-level mean draw/rep paths\n",
    "draw_mean = draw_sum_fund / float(N_SIMS)\n",
    "rep_mean = rep_sum_fund / float(N_SIMS)\n",
    "rows = []\n",
    "for i, fid in enumerate(fund_ids):\n",
    "    for t, qe in enumerate(quarters):\n",
    "        rows.append({\"FundID\": fid, \"quarter_end\": qe, \"draw_mean\": draw_mean[i, t]})\n",
    "fund_draw_path = out_dir / \"sim_fund_draw_mean.csv\"\n",
    "pd.DataFrame(rows).to_csv(fund_draw_path, index=False)\n",
    "print(\"Wrote:\", fund_draw_path)\n",
    "\n",
    "rows = []\n",
    "for i, fid in enumerate(fund_ids):\n",
    "    for t, qe in enumerate(quarters):\n",
    "        rows.append({\"FundID\": fid, \"quarter_end\": qe, \"rep_mean\": rep_mean[i, t]})\n",
    "fund_rep_path = out_dir / \"sim_fund_rep_mean.csv\"\n",
    "pd.DataFrame(rows).to_csv(fund_rep_path, index=False)\n",
    "print(\"Wrote:\", fund_rep_path)\n",
    "\n",
    "# write fund-level end-of-life draw/rep (mean)\n",
    "end_rows = []\n",
    "for i, fid in enumerate(fund_ids):\n",
    "    st = fund_states.get(fid, {})\n",
    "    fe = st.get(\"fund_end_qe\", None)\n",
    "    if pd.notna(fe):\n",
    "        q_idx = int(pd.Period(fe, freq=\"Q\").ordinal - pd.Period(start_qe, freq=\"Q\").ordinal)\n",
    "        q_idx = max(0, min(q_idx-1, len(quarters)-1))\n",
    "    else:\n",
    "        q_idx = len(quarters)-1\n",
    "    end_rows.append({\"FundID\": fid, \"fund_end_qe\": fe, \"draw_end_mean\": draw_mean[i, q_idx], \"rep_end_mean\": rep_mean[i, q_idx]})\n",
    "fund_flow_end = out_dir / \"sim_fund_flow_end_mean.csv\"\n",
    "pd.DataFrame(end_rows).to_csv(fund_flow_end, index=False)\n",
    "print(\"Wrote:\", fund_flow_end)\n",
    "\n",
    "# diagnostic pace check (last N historical vs first N projected; N defaults to full horizon)\n",
    "try:\n",
    "    hist_agg = df.groupby(\"quarter_end\", as_index=False).agg(\n",
    "        hist_draw=(\"Adj Drawdown EUR\", lambda s: pd.to_numeric(s, errors=\"coerce\").abs().sum()),\n",
    "    ).sort_values(\"quarter_end\")\n",
    "    hist_last = hist_agg[hist_agg[\"quarter_end\"] <= start_qe].tail(PACE_CALIB_N_Q)\n",
    "    proj_first = pd.read_csv(out_dir / \"sim_portfolio_series.csv\").head(PACE_CALIB_N_Q)\n",
    "    hist_sum = float(hist_last[\"hist_draw\"].sum())\n",
    "    proj_sum = float(proj_first[\"sim_draw_mean\"].sum())\n",
    "    if proj_sum > 0:\n",
    "        suggested = hist_sum / proj_sum\n",
    "        print(f\"Suggested PACE_SCALE to match history: {suggested:.2f}\")\n",
    "except Exception as e:\n",
    "    print(\"PACE_SCALE suggestion skipped:\", e)\n",
    "\n",
    "\n",
    "# --- Diagnostics summary (last 10 historical vs first 10 projected) ---\n",
    "try:\n",
    "    hist_agg = df.groupby(\"quarter_end\", as_index=False).agg(\n",
    "        hist_draw=(\"Adj Drawdown EUR\", lambda s: pd.to_numeric(s, errors=\"coerce\").abs().sum()),\n",
    "        hist_rep=(\"Adj Repayment EUR\", lambda s: pd.to_numeric(s, errors=\"coerce\").abs().sum()),\n",
    "        hist_nav=(\"NAV Adjusted EUR\", lambda s: pd.to_numeric(s, errors=\"coerce\").abs().sum()),\n",
    "        active_funds=(\"FundID\", \"nunique\"),\n",
    "    ).sort_values(\"quarter_end\")\n",
    "    hist_last = hist_agg[hist_agg[\"quarter_end\"] <= start_qe].tail(10)\n",
    "    proj_first = pd.read_csv(out_dir / \"sim_portfolio_series.csv\").head(10)\n",
    "    active_sim = (nav_mean > 1e-6).sum(axis=0)[: len(proj_first)]\n",
    "    diag = {\n",
    "        \"hist_draw_sum\": float(hist_last[\"hist_draw\"].sum()),\n",
    "        \"proj_draw_sum\": float(proj_first[\"sim_draw_mean\"].sum()),\n",
    "        \"hist_rep_sum\": float(hist_last[\"hist_rep\"].sum()),\n",
    "        \"proj_rep_sum\": float(proj_first[\"sim_rep_mean\"].sum()),\n",
    "        \"hist_nav_avg\": float(hist_last[\"hist_nav\"].mean()),\n",
    "        \"proj_nav_avg\": float(proj_first[\"sim_nav_mean\"].mean()),\n",
    "        \"hist_draw_per_active\": float((hist_last[\"hist_draw\"] / hist_last[\"active_funds\"]).mean()),\n",
    "        \"proj_draw_per_active\": float((proj_first[\"sim_draw_mean\"] / active_sim).mean()),\n",
    "        \"hist_rep_per_active\": float((hist_last[\"hist_rep\"] / hist_last[\"active_funds\"]).mean()),\n",
    "        \"proj_rep_per_active\": float((proj_first[\"sim_rep_mean\"] / active_sim).mean()),\n",
    "    }\n",
    "    pd.DataFrame([diag]).to_csv(out_dir / \"sim_diagnostics_summary.csv\", index=False)\n",
    "    print(\"Wrote:\", out_dir / \"sim_diagnostics_summary.csv\")\n",
    "except Exception as e:\n",
    "    print(\"Diagnostics summary skipped:\", e)\n",
    "\n",
    "# --- Diagnostics by age bucket ---\n",
    "try:\n",
    "    df_diag = df.copy()\n",
    "    df_diag[\"AgeBucket\"] = pd.cut(\n",
    "        pd.to_numeric(df_diag.get(\"Fund_Age_Quarters\"), errors=\"coerce\"),\n",
    "        bins=AGE_BINS_Q,\n",
    "        labels=AGE_LABELS,\n",
    "    )\n",
    "    df_diag[\"draw_flag\"] = pd.to_numeric(df_diag.get(\"Adj Drawdown EUR\"), errors=\"coerce\").abs().fillna(0.0) > 0\n",
    "    hist_p_draw = df_diag.groupby(\"AgeBucket\")[\"draw_flag\"].mean().rename(\"hist_p_draw\")\n",
    "\n",
    "    tp = pd.read_csv(Path(FIT_DIR) / \"timing_probs_selected.csv\")\n",
    "    tp_p = tp.groupby(\"AgeBucket\")[\"p_draw\"].mean().rename(\"p_draw_sel\")\n",
    "\n",
    "    rr = pd.read_csv(Path(FIT_DIR) / \"ratio_fit_selected.csv\")\n",
    "    rr = rr[rr[\"ratio\"] == \"draw_ratio\"].copy()\n",
    "    rr[\"target\"] = rr[\"median_cap\"].fillna(rr[\"data_median\"])\n",
    "    rr[\"n\"] = pd.to_numeric(rr.get(\"n\"), errors=\"coerce\").fillna(0)\n",
    "    rr_age = rr.groupby(\"AgeBucket\").apply(\n",
    "        lambda g: (g[\"target\"] * g[\"n\"]).sum() / g[\"n\"].sum() if g[\"n\"].sum() > 0 else g[\"target\"].mean(),\n",
    "        include_groups=False,\n",
    "    ).rename(\"target_mean\")\n",
    "\n",
    "    out = pd.concat([hist_p_draw, tp_p, rr_age], axis=1).reset_index()\n",
    "    out.to_csv(out_dir / \"sim_diagnostics_by_age.csv\", index=False)\n",
    "    print(\"Wrote:\", out_dir / \"sim_diagnostics_by_age.csv\")\n",
    "except Exception as e:\n",
    "    print(\"Diagnostics by age skipped:\", e)\n",
    "\n",
    "# --- Diagnostics: draw zero reasons by quarter ---\n",
    "try:\n",
    "    total_active = active_count if active_count is not None else None\n",
    "    if total_active is not None:\n",
    "        df_zero = pd.DataFrame({\n",
    "            \"quarter_end\": quarters,\n",
    "            \"active_funds\": total_active,\n",
    "            \"draw_positive\": draw_positive,\n",
    "            \"zero_gate\": zero_gate,\n",
    "            \"zero_target\": zero_target,\n",
    "            \"zero_capacity\": zero_capacity,\n",
    "            \"zero_pace\": zero_pace,\n",
    "        })\n",
    "        # shares\n",
    "        for c in [\"draw_positive\",\"zero_gate\",\"zero_target\",\"zero_capacity\",\"zero_pace\"]:\n",
    "            df_zero[c + \"_share\"] = np.where(df_zero[\"active_funds\"] > 0, df_zero[c] / df_zero[\"active_funds\"], np.nan)\n",
    "        df_zero.to_csv(out_dir / \"draw_zero_reasons_by_quarter.csv\", index=False)\n",
    "        summary = {\n",
    "            \"active_funds\": float(df_zero[\"active_funds\"].sum()),\n",
    "            \"draw_positive\": float(df_zero[\"draw_positive\"].sum()),\n",
    "            \"zero_gate\": float(df_zero[\"zero_gate\"].sum()),\n",
    "            \"zero_target\": float(df_zero[\"zero_target\"].sum()),\n",
    "            \"zero_capacity\": float(df_zero[\"zero_capacity\"].sum()),\n",
    "            \"zero_pace\": float(df_zero[\"zero_pace\"].sum()),\n",
    "        }\n",
    "        df_sum = pd.DataFrame([summary])\n",
    "        df_sum.to_csv(out_dir / \"draw_zero_reasons_summary.csv\", index=False)\n",
    "        print(\"Wrote:\", out_dir / \"draw_zero_reasons_by_quarter.csv\")\n",
    "        print(\"Wrote:\", out_dir / \"draw_zero_reasons_summary.csv\")\n",
    "except Exception as e:\n",
    "    print(\"Draw zero diagnostics skipped:\", e)\n",
    "\n",
    "# --- Diagnostics: repayment gating ---\n",
    "try:\n",
    "    df_repdiag = pd.DataFrame({\n",
    "        \"quarter_end\": quarters,\n",
    "        \"rep_event_count\": rep_event_count,\n",
    "        \"rep_navpos_count\": rep_navpos_count,\n",
    "        \"rep_rr_missing\": rep_rr_missing,\n",
    "        \"rep_prep_nan\": rep_prep_nan,\n",
    "        \"timing_key_missing\": timing_key_missing,\n",
    "    })\n",
    "    df_repdiag.to_csv(out_dir / 'repayment_diagnostics_by_quarter.csv', index=False)\n",
    "    print('Wrote:', out_dir / 'repayment_diagnostics_by_quarter.csv')\n",
    "except Exception as e:\n",
    "    print('Repayment diagnostics skipped:', e)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
